# src/uosai/crawler/uostory_crawler_with_cookie.py
"""
UOS Story ë¹„êµê³¼ í”„ë¡œê·¸ë¨ í¬ë¡¤ëŸ¬
- ë¸Œë¼ìš°ì € ì¿ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œê·¸ì¸ ìƒíƒœ ìœ ì§€
"""

import time
import re
import json
import random
import os
from typing import List, Optional
from urllib.parse import urlencode
from datetime import datetime
from io import BytesIO

import requests
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright
from PIL import Image
import numpy as np
import easyocr
from openai import OpenAI

# =========================
# ì„¤ì •
# =========================

BASE_URL = "https://uostory.uos.ac.kr"
LIST_URL = f"{BASE_URL}/site/reservation/lecture/lectureList"
DETAIL_URL = f"{BASE_URL}/site/reservation/lecture/lectureDetail"
COOKIE_FILE = "uostory_cookies.json"

# ì—¬ëŸ¬ ë©”ë‰´ì—ì„œ í”„ë¡œê·¸ë¨ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±°)
PROGRAM_MENUS = [
    {
        "name": "ë¹„êµê³¼ í”„ë¡œê·¸ë¨",
        "menuid": "001003002001",
        "submode": "lecture",
        "reservegroupid": "1",
        "rectype": "L"
    },
    {
        "name": "ì·¨ì—… í”„ë¡œê·¸ë¨",
        "menuid": "001002002002",
        "reservegroupid": "1",
        "rectype": "J"
    }
]

REQUEST_SLEEP_MIN = 5.0  # ìµœì†Œ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
REQUEST_SLEEP_MAX = 10.0  # ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
RECENT_WINDOW_PER_MENU = 50  # ê° ë©”ë‰´ë³„ ìˆ˜ì§‘í•  í”„ë¡œê·¸ë¨ ê°œìˆ˜
MAX_PAGES = 10  # ìµœëŒ€ í˜ì´ì§€ ìˆ˜
CONNECT_TIMEOUT = 10
READ_TIMEOUT = 20

# =========================
# OpenAI API ì´ˆê¸°í™”
# =========================
from dotenv import load_dotenv
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    print("âš ï¸ WARNING: OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("OCR í…ìŠ¤íŠ¸ ì •ë¦¬ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    OPENAI_CLIENT = None
else:
    OPENAI_CLIENT = OpenAI(api_key=OPENAI_API_KEY)
    print("âœ… OpenAI API ì´ˆê¸°í™” ì™„ë£Œ")

# =========================
# EasyOCR ì´ˆê¸°í™”
# =========================
# ì „ì—­ìœ¼ë¡œ í•œ ë²ˆë§Œ ì´ˆê¸°í™” (í•œê¸€, ì˜ì–´ ì§€ì›)
log_print = lambda msg: print(f"[{datetime.now():%Y-%m-%d %H:%M:%S}] {msg}")
log_print("ğŸ”§ EasyOCR ì´ˆê¸°í™” ì¤‘... (ìµœì´ˆ ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)")
OCR_READER = easyocr.Reader(['ko', 'en'], gpu=False, verbose=False)
log_print("âœ… EasyOCR ì´ˆê¸°í™” ì™„ë£Œ")

# ì¿ í‚¤ ë¡œë“œ (cookies.json íŒŒì¼ì—ì„œ)
COOKIE_FILE_PATH = "cookies.json"

if not os.path.exists(COOKIE_FILE_PATH):
    print(f"ERROR: ì¿ í‚¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {COOKIE_FILE_PATH}")
    print("cookies.json íŒŒì¼ì„ ìƒì„±í•´ì£¼ì„¸ìš”.")
    exit(1)

try:
    with open(COOKIE_FILE_PATH, 'r', encoding='utf-8') as f:
        RAW_COOKIES = json.load(f)
    print(f"ì¿ í‚¤ ë¡œë“œ ì™„ë£Œ: {len(RAW_COOKIES)}ê°œ")
except json.JSONDecodeError as e:
    print(f"ERROR: ì¿ í‚¤ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
    print(f"{COOKIE_FILE_PATH} íŒŒì¼ì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    exit(1)
except Exception as e:
    print(f"ERROR: ì¿ í‚¤ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    exit(1)

# requestsìš© ì¿ í‚¤ ë”•ì…”ë„ˆë¦¬ ìƒì„±
COOKIES = {cookie['name']: cookie['value'] for cookie in RAW_COOKIES}

# =========================
# ìœ í‹¸ë¦¬í‹°
# =========================

def log(msg: str) -> None:
    print(f"[{datetime.now():%Y-%m-%d %H:%M:%S}] {msg}")


def get_headers() -> dict:
    return {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9",
        "Referer": BASE_URL,
    }


def clean_content(text: str) -> str:
    """ë‚´ìš© ì •ë¦¬: ê³¼ë„í•œ ì¤„ë°”ê¿ˆ ì œê±° ë° ë¬¸ë‹¨ ì •ë¦¬"""
    if not text:
        return ""

    # ì—°ì†ëœ ê³µë°±/íƒ­ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ
    text = re.sub(r'[ \t]+', ' ', text)

    # ì¤„ë°”ê¿ˆ ì •ë¦¬
    # 1. ë‹¨ì¼ ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ (ë¬¸ì¥ ì—°ê²°)
    text = re.sub(r'([^\n])\n([^\n])', r'\1 \2', text)

    # 2. ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ ìµœëŒ€ 2ê°œë¡œ (ë¬¸ë‹¨ êµ¬ë¶„)
    text = re.sub(r'\n{2,}', '\n\n', text)

    # 3. ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()

    return text


def classify_program_categories(title: str, content: str) -> List[str]:
    """í”„ë¡œê·¸ë¨ ì œëª©ê³¼ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜ (ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)"""
    text = (title + " " + content).lower()
    categories = []

    # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ íŒ¨í„´
    patterns = {
        "ê³µëª¨ì „": r"ê³µëª¨ì „|ê²½ì§„ëŒ€íšŒ|ëŒ€íšŒ|ì½˜í…ŒìŠ¤íŠ¸|contest|competition",
        "ë©˜í† ë§": r"ë©˜í† ë§|ë©˜í† |ë©˜í‹°|ì½”ì¹­|ìƒë‹´",
        "ë´‰ì‚¬": r"ë´‰ì‚¬|ìì›ë´‰ì‚¬|ì‚¬íšŒê³µí—Œ|volunteer",
        "ì·¨ì—…": r"ì·¨ì—…|ì±„ìš©|ë©´ì ‘|ì´ë ¥ì„œ|ìê¸°ì†Œê°œì„œ|ì»¤ë¦¬ì–´|ì¸í„´|job|career|employment|ì…ì‚¬",
        "íƒë°©": r"íƒë°©|ê²¬í•™|ë°©ë¬¸|íˆ¬ì–´|ë‹µì‚¬|field.?trip",
        "íŠ¹ê°•": r"íŠ¹ê°•|ê°•ì—°|ì„¸ë¯¸ë‚˜|ì›Œí¬ìƒµ|êµìœ¡|lecture|seminar|workshop",
    }

    # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë§¤ì¹­ í™•ì¸ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)
    for category, pattern in patterns.items():
        if re.search(pattern, text):
            categories.append(category)

    # ë§¤ì¹­ë˜ëŠ” ê²ƒì´ ì—†ìœ¼ë©´ ê¸°íƒ€
    if not categories:
        categories.append("ê¸°íƒ€")

    log(f"âœ… ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜: {', '.join(categories)}")
    return categories


def clean_ocr_text_with_ai(ocr_text: str) -> Optional[str]:
    """OpenAI APIë¡œ OCR í…ìŠ¤íŠ¸ ì •ë¦¬ ë° êµ¬ì¡°í™”"""
    if not OPENAI_CLIENT:
        log("âš ï¸ OpenAI API ë¯¸ì„¤ì • - ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜")
        return ocr_text

    try:
        log(f"ğŸ¤– AIë¡œ í…ìŠ¤íŠ¸ ì •ë¦¬ ì¤‘... ({len(ocr_text)} ê¸€ì)")

        prompt = f"""ë‹¤ìŒì€ í¬ìŠ¤í„°ì—ì„œ OCRë¡œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
ì˜¤íƒ€ì™€ ë„ì–´ì“°ê¸°ë¥¼ ìˆ˜ì •í•˜ê³ , í•µì‹¬ ì •ë³´ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

ê°€ëŠ¥í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±:
- í–‰ì‚¬ëª…/í”„ë¡œê·¸ë¨ëª…: (ìˆìœ¼ë©´ ì‘ì„±)
- ì¼ì‹œ: (ìˆìœ¼ë©´ ì‘ì„±)
- ì¥ì†Œ: (ìˆìœ¼ë©´ ì‘ì„±)
- ê°•ì‚¬/ì£¼ì œ: (ìˆìœ¼ë©´ ì‘ì„±)
- ì‹ ì²­ë°©ë²•: (ìˆìœ¼ë©´ ì‘ì„±)
- ë¬¸ì˜ì²˜: (ìˆìœ¼ë©´ ì‘ì„±)
- ê¸°íƒ€ ë‚´ìš©: (ìœ„ì— í¬í•¨ë˜ì§€ ì•Šì€ ì¤‘ìš” ì •ë³´)

ì—†ëŠ” í•­ëª©ì€ ìƒëµí•˜ê³ , ìˆëŠ” ì •ë³´ë§Œ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.


OCR í…ìŠ¤íŠ¸:
{ocr_text}
"""

        response = OPENAI_CLIENT.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ OCR í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì˜¤íƒ€ë¥¼ ìˆ˜ì •í•˜ê³  ì •ë³´ë¥¼ êµ¬ì¡°í™”í•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )

        cleaned_text = response.choices[0].message.content.strip()
        log(f"âœ… AI ì •ë¦¬ ì™„ë£Œ: {len(cleaned_text)} ê¸€ì")
        return cleaned_text

    except Exception as e:
        log(f"âŒ AI ì •ë¦¬ ì‹¤íŒ¨: {e}")
        log("ì›ë³¸ OCR í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤")
        return ocr_text


def extract_text_from_image(image_url: str) -> Optional[str]:
    """ì´ë¯¸ì§€ URLì—ì„œ OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        log(f"ğŸ–¼ï¸  ì´ë¯¸ì§€ OCR ì‹œì‘: {image_url}")

        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        response = requests.get(
            image_url,
            headers=get_headers(),
            cookies=COOKIES,
            timeout=(CONNECT_TIMEOUT, READ_TIMEOUT)
        )

        if response.status_code != 200:
            log(f"âŒ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: HTTP {response.status_code}")
            return None

        # PIL Imageë¡œ ë³€í™˜
        image = Image.open(BytesIO(response.content))

        # RGBë¡œ ë³€í™˜ (RGBA ë“±ì˜ ê²½ìš° ëŒ€ë¹„)
        if image.mode != 'RGB':
            image = image.convert('RGB')

        # numpy arrayë¡œ ë³€í™˜
        image_np = np.array(image)

        # OCR ì‹¤í–‰
        log(f"ğŸ” OCR ì‹¤í–‰ ì¤‘...")
        results = OCR_READER.readtext(image_np)

        # ê²°ê³¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì‹ ë¢°ë„ 0.3 ì´ìƒë§Œ)
        extracted_lines = []
        for (bbox, text, confidence) in results:
            if confidence > 0.3:
                extracted_lines.append(text)

        extracted_text = '\n'.join(extracted_lines)

        log(f"âœ… OCR ì™„ë£Œ: {len(extracted_lines)}ê°œ í…ìŠ¤íŠ¸ ë¼ì¸ ì¶”ì¶œ")
        return extracted_text.strip()

    except Exception as e:
        log(f"âŒ OCR ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None


def parse_departments(dept_text: str) -> list:
    """í•™ê³¼ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ í•™ê³¼ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    if not dept_text:
        return ['ì œí•œì—†ìŒ']

    # "í•™ë…„" í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í•™ê³¼ ë¶€ë¶„ë§Œ ì¶”ì¶œ
    # ì˜ˆ: "ì œí•œì—†ìŒí•™ë…„ : ëŒ€í•™ì›ìƒ" â†’ "ì œí•œì—†ìŒ"ë§Œ ì¶”ì¶œ
    dept_only = re.split(r'í•™ë…„', dept_text)[0].strip()

    # ì‰¼í‘œë‚˜ / ë¡œ êµ¬ë¶„ëœ í•™ê³¼ë“¤ì„ ë¶„ë¦¬
    departments = re.split(r'[,/]', dept_only)

    result = []
    for dept in departments:
        dept = dept.strip()
        # ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œ ì œê±°
        dept = re.sub(r'[:ï¼š\s]+$', '', dept)  # ëì˜ ì½œë¡ , ê³µë°± ì œê±°

        if dept and dept not in ['ì œí•œì—†ìŒ', '']:
            result.append(dept)

    return result if result else ['ì œí•œì—†ìŒ']


def parse_grades(grade_text: str) -> list:
    """í•™ë…„ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ í•™ë…„ ì½”ë“œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    ë§¤í•‘:
    - 0: ì œí•œì—†ìŒ/ì „ì²´
    - 1~5: 1~5í•™ë…„
    - 6: ì¡¸ì—…ìƒ
    - 7: ëŒ€í•™ì›ìƒ
    """
    if not grade_text:
        return [0]

    grades = []

    # ìˆ«ì í•™ë…„ ì¶”ì¶œ (1í•™ë…„, 2í•™ë…„ ë“±)
    numeric_grades = re.findall(r'(\d+)í•™ë…„', grade_text)
    for g in numeric_grades:
        grade_num = int(g)
        if 1 <= grade_num <= 5:
            grades.append(grade_num)

    # ì¡¸ì—…ìƒ ì²´í¬
    if re.search(r'ì¡¸ì—…ìƒ?', grade_text):
        grades.append(6)

    # ëŒ€í•™ì›ìƒ ì²´í¬
    if re.search(r'ëŒ€í•™ì›ìƒ?', grade_text):
        grades.append(7)

    # ì œí•œì—†ìŒ/ì „ì²´ ì²´í¬
    if re.search(r'ì œí•œ\s*ì—†ìŒ|ì „ì²´', grade_text) or not grades:
        return [0]

    return grades


def print_db_insert_info(data: dict) -> None:
    """DB ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ì €ì¥ë  ë°ì´í„°ë¥¼ ì½˜ì†”ì— ì¶œë ¥"""
    print("\n" + "="*100)
    print("ğŸ“Š DB ì €ì¥ ë°ì´í„° (ì½˜ì†” ì¶œë ¥)")
    print("="*100)

    # í•™ê³¼ íŒŒì‹±
    departments = parse_departments(data.get('target_department', ''))

    # í•™ë…„ íŒŒì‹± (int ì½”ë“œ: 0=ì „ì²´, 1-5=í•™ë…„, 6=ì¡¸ì—…ìƒ, 7=ëŒ€í•™ì›)
    grades = parse_grades(data.get('target_grade', ''))

    # ì¹´í…Œê³ ë¦¬
    categories = data.get('categories', ['ê¸°íƒ€'])

    # DB ì €ì¥ìš© ë°ì´í„° êµ¬ì¡° (IDëŠ” ìë™ ìƒì„±)
    db_data = {
        "title": data['title'],
        "category": categories,  # ë°°ì—´
        "link": data.get('link', ''),
        "department": departments,  # ë°°ì—´
        "grade": grades,  # ë°°ì—´ (int ì½”ë“œ)
        "content": data.get('content', ''),
        "app_start_date": data.get('application_start'),
        "app_end_date": data.get('application_end')
    }

    print("\n[ì½˜ì†” ì¶œë ¥ - JSON í˜•ì‹]")
    print(json.dumps(db_data, ensure_ascii=False, indent=2))

    print("\n" + "="*100)
    print()


def print_program_info(data: dict) -> None:
    print("\n" + "="*80)
    print(f"í”„ë¡œê·¸ë¨ ID: {data['program_id']}")
    print(f"ì œëª©: {data['title']}")
    print(f"ë§í¬: {data.get('link', '')}")
    print("-"*80)

    if data.get('categories'):
        print(f"ì¹´í…Œê³ ë¦¬: {', '.join(data['categories'])}")
    elif data.get('category'):
        print(f"ì¹´í…Œê³ ë¦¬: {data['category']}")
    if data.get('target_department'):
        print(f"ëŒ€ìƒí•™ê³¼: {data['target_department']}")
    if data.get('target_grade'):
        print(f"ëŒ€ìƒí•™ë…„: {data['target_grade']}")
    if data.get('selection_method'):
        print(f"ì„ ë°œë°©ì‹: {data['selection_method']}")
    if data.get('capacity'):
        print(f"ëª¨ì§‘ì¸ì›: {data['capacity']}ëª…")
    if data.get('location'):
        print(f"ì¥ì†Œ: {data['location']}")

    print("-"*80)

    if data.get('application_start'):
        print(f"ì‹ ì²­ ì‹œì‘: {data['application_start']}")
    if data.get('application_end'):
        print(f"ì‹ ì²­ ë§ˆê°: {data['application_end']}")
    if data.get('operation_start'):
        print(f"ìš´ì˜ ì‹œì‘: {data['operation_start']}")
    if data.get('operation_end'):
        print(f"ìš´ì˜ ë§ˆê°: {data['operation_end']}")
    if data.get('status'):
        print(f"ìƒíƒœ: {data['status']}")

    print("-"*80)

    if data.get('content'):
        cleaned = clean_content(data['content'])
        print(f"ë‚´ìš©:\n{cleaned}")

    print("="*80 + "\n")


# =========================
# ëª©ë¡ ìˆ˜ì§‘
# =========================

def extract_program_ids_from_html(html: str) -> List[int]:
    soup = BeautifulSoup(html, "html.parser")
    program_ids = []

    for link in soup.find_all("a", href=True):
        href = link.get("href", "")
        if "lectureDetail" in href and "lecturegroupid" in href:
            match = re.search(r"lecturegroupid[=](\d+)", href)
            if match:
                program_ids.append(int(match.group(1)))

    seen = set()
    result = []
    for pid in program_ids:
        if pid not in seen:
            seen.add(pid)
            result.append(pid)

    return result


def collect_program_ids(limit_per_menu: int = RECENT_WINDOW_PER_MENU, max_pages: int = MAX_PAGES) -> List[int]:
    """ì—¬ëŸ¬ ë©”ë‰´ì—ì„œ í”„ë¡œê·¸ë¨ ID ìˆ˜ì§‘ (ê° ë©”ë‰´ë³„ë¡œ limit_per_menuê°œì”©)"""
    all_program_ids = []
    global_seen = set()  # ì „ì—­ ì¤‘ë³µ ì²´í¬

    # ê° ë©”ë‰´ë³„ë¡œ í¬ë¡¤ë§
    for menu in PROGRAM_MENUS:
        log(f"\n{'='*60}")
        log(f"[{menu['name']}] ë©”ë‰´ í¬ë¡¤ë§ ì‹œì‘ (ëª©í‘œ: {limit_per_menu}ê°œ)")
        log(f"{'='*60}")

        menu_program_ids = []  # ì´ ë©”ë‰´ì—ì„œ ìˆ˜ì§‘í•œ IDë“¤

        for page in range(1, max_pages + 1):
            try:
                log(f"[{menu['name']}] í˜ì´ì§€ {page} ìš”ì²­ ì¤‘...")

                # í˜ì´ì§€ë³„ íŒŒë¼ë¯¸í„°
                params = menu.copy()
                params.pop('name')  # nameì€ íŒŒë¼ë¯¸í„°ê°€ ì•„ë‹˜
                params["currentpage"] = str(page)
                params["viewtype"] = "L"
                params["thumbnail"] = "Y"

                response = requests.get(
                    LIST_URL,
                    params=params,
                    headers=get_headers(),
                    cookies=COOKIES,
                    timeout=(CONNECT_TIMEOUT, READ_TIMEOUT),
                    allow_redirects=True
                )

                if response.status_code != 200:
                    log(f"âŒ í˜ì´ì§€ {page} HTTP {response.status_code}")
                    break

                if "login" in response.url.lower() or "sso" in response.url.lower():
                    log("âš ï¸ ë¡œê·¸ì¸ í•„ìš” - ì¿ í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”")
                    break

                page_program_ids = extract_program_ids_from_html(response.text)

                # ì´ ë©”ë‰´ì—ì„œ ì•„ì§ ì•ˆ ë³¸ í”„ë¡œê·¸ë¨ë§Œ ì¶”ê°€
                new_count = 0
                duplicate_count = 0
                for pid in page_program_ids:
                    if pid in global_seen:
                        duplicate_count += 1  # ë‹¤ë¥¸ ë©”ë‰´ì—ì„œ ì´ë¯¸ ìˆ˜ì§‘í•¨
                    elif len(menu_program_ids) < limit_per_menu:
                        menu_program_ids.append(pid)
                        global_seen.add(pid)
                        new_count += 1

                log(f"[{menu['name']}] í˜ì´ì§€ {page}: {len(page_program_ids)}ê°œ ë°œê²¬, "
                    f"{new_count}ê°œ ì‹ ê·œ, {duplicate_count}ê°œ ì¤‘ë³µ "
                    f"(ë©”ë‰´ ëˆ„ì : {len(menu_program_ids)}/{limit_per_menu})")

                # ì´ ë©”ë‰´ì—ì„œ ëª©í‘œ ê°œìˆ˜ ë„ë‹¬
                if len(menu_program_ids) >= limit_per_menu:
                    log(f"[{menu['name']}] ëª©í‘œ {limit_per_menu}ê°œ ë‹¬ì„± - ë‹¤ìŒ ë©”ë‰´ë¡œ")
                    break

                # ë‹¤ìŒ í˜ì´ì§€ ìš”ì²­ ì „ ëŒ€ê¸°
                if page < max_pages:
                    time.sleep(2)

            except Exception as e:
                log(f"âŒ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                break

        # ì´ ë©”ë‰´ì˜ ê²°ê³¼ë¥¼ ì „ì²´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        all_program_ids.extend(menu_program_ids)
        log(f"[{menu['name']}] ìµœì¢…: {len(menu_program_ids)}ê°œ ìˆ˜ì§‘")

    log(f"\n{'='*60}")
    log(f"âœ… ì „ì²´ ë©”ë‰´ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_program_ids)}ê°œ í”„ë¡œê·¸ë¨ ë°œê²¬")
    log(f"{'='*60}\n")
    return all_program_ids


# =========================
# ìƒì„¸ í˜ì´ì§€ íŒŒì‹±
# =========================

def parse_date_range(text: str) -> tuple[Optional[str], Optional[str]]:
    """ë‚ ì§œ ë²”ìœ„ íŒŒì‹± (ì‹œê°„ í¬í•¨ ê°€ëŠ¥)

    ì˜ˆ: "2025-11-07 10:00:00 ~ 2025-11-14 23:59:00" â†’ ("2025-11-07", "2025-11-14")
    """
    if not text:
        return None, None

    # íŒ¨í„´ 1: ë‚ ì§œ+ì‹œê°„ ë²”ìœ„ (YYYY-MM-DD HH:MM:SS ~ YYYY-MM-DD HH:MM:SS)
    match = re.search(r'(\d{4}-\d{2}-\d{2})\s+\d{2}:\d{2}:\d{2}\s*~\s*(\d{4}-\d{2}-\d{2})\s+\d{2}:\d{2}:\d{2}', text)
    if match:
        return match.group(1), match.group(2)

    # íŒ¨í„´ 2: ë‚ ì§œë§Œ ë²”ìœ„ (YYYY-MM-DD ~ YYYY-MM-DD)
    match = re.search(r'(\d{4}-\d{2}-\d{2})\s*~\s*(\d{4}-\d{2}-\d{2})', text)
    if match:
        return match.group(1), match.group(2)

    # íŒ¨í„´ 3: ë‹¨ì¼ ë‚ ì§œ+ì‹œê°„ (YYYY-MM-DD HH:MM:SS)
    match = re.search(r'(\d{4}-\d{2}-\d{2})\s+\d{2}:\d{2}:\d{2}', text)
    if match:
        return match.group(1), match.group(1)

    # íŒ¨í„´ 4: ë‹¨ì¼ ë‚ ì§œ (YYYY-MM-DD)
    match = re.search(r'(\d{4}-\d{2}-\d{2})', text)
    if match:
        return match.group(1), match.group(1)

    return None, None


def fetch_program_html_with_playwright(program_id: int) -> Optional[str]:
    """Playwrightë¡œ ìë°”ìŠ¤í¬ë¦½íŠ¸ ë Œë”ë§ í›„ HTML ê°€ì ¸ì˜¤ê¸°"""
    try:
        params = {
            "menuid": "001003002001",
            "reservegroupid": "1",
            "viewtype": "L",
            "rectype": "L",
            "thumbnail": "Y",
            "lecturegroupid": str(program_id)
        }

        url = f"{DETAIL_URL}?{urlencode(params)}"
        log(f"ğŸ“¡ í”„ë¡œê·¸ë¨ {program_id} Playwrightë¡œ ìš”ì²­ ì¤‘...")

        with sync_playwright() as p:
            # ë¸Œë¼ìš°ì € ì˜µì…˜ ì„¤ì • (ë´‡ ê°ì§€ ìš°íšŒ)
            browser = p.chromium.launch(
                headless=True,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-dev-shm-usage',
                    '--no-sandbox'
                ]
            )

            # ì»¨í…ìŠ¤íŠ¸ ì„¤ì • (ì¼ë°˜ ë¸Œë¼ìš°ì €ì²˜ëŸ¼)
            context = browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080},
                locale='ko-KR',
                timezone_id='Asia/Seoul'
            )

            # ì›ë³¸ ì¿ í‚¤ ê·¸ëŒ€ë¡œ ì‚¬ìš© (domain, path, expires ë“± ëª¨ë‘ í¬í•¨)
            playwright_cookies = []
            for cookie in RAW_COOKIES:
                playwright_cookie = {
                    "name": cookie['name'],
                    "value": cookie['value'],
                    "domain": cookie.get('domain', '.uos.ac.kr'),
                    "path": cookie.get('path', '/'),
                }

                # expires ìˆìœ¼ë©´ ì¶”ê°€
                if 'expires' in cookie and cookie['expires'] != -1:
                    playwright_cookie['expires'] = cookie['expires']

                # httpOnly, secure í”Œë˜ê·¸
                if 'httpOnly' in cookie:
                    playwright_cookie['httpOnly'] = cookie['httpOnly']
                if 'secure' in cookie:
                    playwright_cookie['secure'] = cookie['secure']

                # sameSite
                if 'sameSite' in cookie:
                    playwright_cookie['sameSite'] = cookie['sameSite']

                playwright_cookies.append(playwright_cookie)

            context.add_cookies(playwright_cookies)
            log(f"ğŸª {len(playwright_cookies)}ê°œ ì¿ í‚¤ ì„¤ì • ì™„ë£Œ")

            page = context.new_page()

            # í˜ì´ì§€ ë¡œë“œ (ë” ê¸´ íƒ€ì„ì•„ì›ƒ)
            try:
                page.goto(url, wait_until="domcontentloaded", timeout=30000)

                # ì¶”ê°€ ëŒ€ê¸° (ìë°”ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ëŒ€ê¸°)
                page.wait_for_timeout(3000)

                # ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ ë˜ì—ˆëŠ”ì§€ í™•ì¸
                current_url = page.url
                if "login" in current_url.lower() or "sso" in current_url.lower():
                    log(f"âš ï¸ ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ë¨: {current_url}")
                    log("ì¿ í‚¤ê°€ ë§Œë£Œë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. uostory_login.pyë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
                    browser.close()
                    return None

            except Exception as e:
                log(f"âš ï¸ í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ: {e}")

            # HTML ê°€ì ¸ì˜¤ê¸°
            html = page.content()

            browser.close()

        log(f"âœ… í”„ë¡œê·¸ë¨ {program_id} ë¡œë“œ ì„±ê³µ")
        return html

    except Exception as e:
        log(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None


def parse_program_fields(html: str, program_id: int) -> Optional[dict]:
    soup = BeautifulSoup(html, "html.parser")

    # ì œëª© ì¶”ì¶œ (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
    title = ""

    # íŒ¨í„´ 1: id="lecturetitle" (ìƒì„¸ í˜ì´ì§€)
    title_el = soup.select_one("#lecturetitle")
    if title_el:
        title = title_el.get_text(strip=True)

    # íŒ¨í„´ 2: page_title í´ë˜ìŠ¤
    if not title:
        title_el = soup.select_one("h2.page_title") or soup.select_one("h3")
        if title_el:
            title = title_el.get_text(strip=True)

    # íŒ¨í„´ 3: í…Œì´ë¸”ì—ì„œ "ê³¼ì •ëª…" ì°¾ê¸°
    if not title:
        for tr in soup.select("tr"):
            th = tr.select_one("th")
            if th and "ê³¼ì •ëª…" in th.get_text(strip=True):
                td = tr.select_one("td")
                if td:
                    link = td.select_one("a")
                    title = link.get_text(strip=True) if link else td.get_text(strip=True)
                    break

    if not title:
        log(f"âš ï¸ Program {program_id}: ì œëª© ì—†ìŒ")
        return None

    fields = {}

    # trans_thead ìŠ¤íƒ€ì¼
    for tr in soup.select("tr.trans_thead"):
        th = tr.select_one("th")
        td = tr.select_one("td")
        if not th or not td:
            continue

        th_text = th.get_text(strip=True)
        td_text = td.get_text(strip=True)

        if "ëŒ€ìƒ" in th_text:
            dept_match = re.search(r'í•™ê³¼\s*[:ï¼š]\s*([^\n]+)', td_text)
            grade_match = re.search(r'í•™ë…„\s*[:ï¼š]\s*([^\n]+)', td_text)
            fields["target_department"] = dept_match.group(1).strip() if dept_match else None
            fields["target_grade"] = grade_match.group(1).strip() if grade_match else None
        elif "ì„ ë°œë°©ì‹" in th_text or "ì„ ë°œ" in th_text:
            fields["selection_method"] = td_text
        elif "ëª¨ì§‘ì¸ì›" in th_text or "ì¸ì›" in th_text:
            num_match = re.search(r'(\d+)', td_text)
            fields["capacity"] = int(num_match.group(1)) if num_match else None
        elif "ì¥ì†Œ" in th_text:
            fields["location"] = td_text
        elif "ì¹´í…Œê³ ë¦¬" in th_text or "í•µì‹¬ì—­ëŸ‰" in th_text:
            fields["category"] = td_text

    # ì¼ë°˜ í…Œì´ë¸”
    for tr in soup.select("tr"):
        th = tr.select_one("th")
        td = tr.select_one("td")
        if not th or not td:
            continue

        th_text = th.get_text(strip=True)
        td_text = td.get_text(strip=True)

        if "ì‹ ì²­ê¸°ê°„" in th_text:
            start, end = parse_date_range(td_text)
            fields["application_start"] = start
            fields["application_end"] = end
        elif "ìš´ì˜ê¸°ê°„" in th_text:
            start, end = parse_date_range(td_text)
            fields["operation_start"] = start
            fields["operation_end"] = end
        elif "ëŒ€ìƒí•™ê³¼" in th_text and "target_department" not in fields:
            fields["target_department"] = td_text
        elif "ëŒ€ìƒí•™ë…„" in th_text and "target_grade" not in fields:
            fields["target_grade"] = td_text

    # ë³¸ë¬¸ ì°¾ê¸°: "ìƒì„¸ë‚´ìš©" th ë‹¤ìŒì˜ td
    content_el = None
    content = ""

    # "ìƒì„¸ë‚´ìš©" thë¥¼ ì°¾ê³  ê·¸ ë‹¤ìŒ trì˜ tdë¥¼ ê°€ì ¸ì˜¤ê¸°
    for th in soup.find_all('th'):
        if 'ìƒì„¸ë‚´ìš©' in th.get_text(strip=True):
            # ìƒì„¸ë‚´ìš© thê°€ ìˆëŠ” trì˜ ë‹¤ìŒ tr ì°¾ê¸°
            parent_tr = th.find_parent('tr')
            if parent_tr:
                next_tr = parent_tr.find_next_sibling('tr')
                if next_tr:
                    content_el = next_tr.find('td')
                    break

    # í´ë°±: "ìƒì„¸ë‚´ìš©"ì„ ëª» ì°¾ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹
    if not content_el:
        content_el = soup.select_one("td[colspan='10']") or soup.select_one("tbody td")

    if content_el:
        content = content_el.get_text("\n", strip=True)

    # ë‚´ìš© ì •ë¦¬
    content = clean_content(content)

    # ğŸ–¼ï¸ ë³¸ë¬¸ ë‚´ ì´ë¯¸ì§€ ì°¾ê¸° ë° OCR ì‹¤í–‰
    image_urls = []

    if content_el:
        # "ìƒì„¸ë‚´ìš©" ì•„ë˜ td ì•ˆì˜ ëª¨ë“  ì´ë¯¸ì§€ ì°¾ê¸°
        for img in content_el.find_all('img'):
            img_src = img.get('src', '')
            if not img_src:
                continue

            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if img_src.startswith('/'):
                img_url = BASE_URL + img_src
            elif img_src.startswith('http'):
                img_url = img_src
            else:
                img_url = BASE_URL + '/' + img_src

            # ì¤‘ë³µ ì œê±°
            if img_url not in image_urls:
                image_urls.append(img_url)

    if image_urls:
        log(f"ğŸ“¸ ë³¸ë¬¸ ë‚´ ì´ë¯¸ì§€ {len(image_urls)}ê°œ ë°œê²¬")

    # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ OCR ì‹¤í–‰ + AI ì •ë¦¬
    ocr_texts = []
    for idx, img_url in enumerate(image_urls, 1):
        log(f"  [{idx}/{len(image_urls)}] ì´ë¯¸ì§€ OCR ì²˜ë¦¬ ì¤‘...")
        ocr_text = extract_text_from_image(img_url)
        if ocr_text:
            # OpenAI APIë¡œ í…ìŠ¤íŠ¸ ì •ë¦¬
            cleaned_text = clean_ocr_text_with_ai(ocr_text)
            if cleaned_text:
                ocr_texts.append(f"[ì´ë¯¸ì§€ {idx} ì •ë³´]\n{cleaned_text}")

    # OCR ê²°ê³¼ë¥¼ ë³¸ë¬¸ì— ì¶”ê°€
    if ocr_texts:
        log(f"âœ… {len(ocr_texts)}ê°œ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ë¦¬ ì™„ë£Œ")
        content = content + "\n\n" + "\n\n".join(ocr_texts)
        content = clean_content(content)
    elif image_urls:
        log(f"âš ï¸ ì´ë¯¸ì§€ëŠ” ìˆì§€ë§Œ OCRë¡œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")

    # ğŸ·ï¸ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
    categories = classify_program_categories(title, content)

    # ìƒíƒœ
    status = "ëª¨ì§‘ì¤‘"
    if fields.get("application_end"):
        try:
            end_date = datetime.strptime(fields["application_end"], "%Y-%m-%d")
            if end_date < datetime.now():
                status = "ë§ˆê°"
        except:
            pass

    return {
        "program_id": program_id,
        "title": title,
        "categories": categories,  # ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ (ë¦¬ìŠ¤íŠ¸)
        "content": content,
        "target_department": fields.get("target_department"),
        "target_grade": fields.get("target_grade"),
        "selection_method": fields.get("selection_method"),
        "capacity": fields.get("capacity"),
        "location": fields.get("location"),
        "application_start": fields.get("application_start"),
        "application_end": fields.get("application_end"),
        "operation_start": fields.get("operation_start"),
        "operation_end": fields.get("operation_end"),
        "status": status,
        "posted_date": datetime.now().strftime("%Y-%m-%d"),
    }


def process_one_program(program_id: int) -> Optional[dict]:
    html = fetch_program_html_with_playwright(program_id)
    if not html:
        return None

    parsed = parse_program_fields(html, program_id)
    if not parsed:
        return None

    params = {
        "menuid": "001003002001",
        "reservegroupid": "1",
        "viewtype": "L",
        "rectype": "L",
        "thumbnail": "Y",
        "lecturegroupid": str(program_id)
    }
    parsed["link"] = f"{DETAIL_URL}?{urlencode(params)}"

    log(f"âœ… íŒŒì‹± ì™„ë£Œ: {parsed['title'][:30]}...")
    return parsed


# =========================
# ë©”ì¸
# =========================

def main():
    log("UOS Story ë¹„êµê³¼ í”„ë¡œê·¸ë¨ í¬ë¡¤ëŸ¬")
    print()

    program_ids = collect_program_ids(limit_per_menu=RECENT_WINDOW_PER_MENU)

    if not program_ids:
        log("í”„ë¡œê·¸ë¨ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
        return 1

    log(f"{len(program_ids)}ê°œ í”„ë¡œê·¸ë¨ ì²˜ë¦¬")
    log(f"ID: {program_ids}")
    print()

    collected = []

    for idx, pid in enumerate(program_ids, 1):
        log(f"[{idx}/{len(program_ids)}] í”„ë¡œê·¸ë¨ {pid} ì²˜ë¦¬ ì¤‘...")

        data = process_one_program(pid)
        if data:
            collected.append(data)
            # ìƒì„¸ ì •ë³´ ì¶œë ¥
            print_program_info(data)
            # DB ì €ì¥ìš© ë°ì´í„° ì¶œë ¥
            print_db_insert_info(data)

        if idx < len(program_ids):
            sleep_time = random.uniform(REQUEST_SLEEP_MIN, REQUEST_SLEEP_MAX)
            log(f"{sleep_time:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
            time.sleep(sleep_time)

    log(f"ì´ {len(collected)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")

    output_file = "uostory_programs.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(collected, f, ensure_ascii=False, indent=2)
    log(f"{output_file}ì— ì €ì¥")

    log("ì™„ë£Œ!")
    return 0


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        log("\nâš ï¸ ì¤‘ë‹¨ë¨")
        exit(0)
    except Exception as e:
        log(f"âŒ ERROR: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
