# uostory_crawler.py
"""
UOS Story ë¹„êµê³¼ í”„ë¡œê·¸ë¨ í¬ë¡¤ëŸ¬
- ë¸Œë¼ìš°ì € ì¿ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œê·¸ì¸ ìƒíƒœ ìœ ì§€
"""

import time
import re
import json
import random
import os
from typing import List, Optional
from urllib.parse import urlencode
from datetime import datetime
from io import BytesIO

import requests
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright
from PIL import Image
import numpy as np
import easyocr
from openai import OpenAI
import mysql.connector
from mysql.connector import Error

# =========================
# ì„¤ì •
# =========================

BASE_URL = "https://uostory.uos.ac.kr"
LIST_URL = f"{BASE_URL}/site/reservation/lecture/lectureList"
DETAIL_URL = f"{BASE_URL}/site/reservation/lecture/lectureDetail"
COOKIE_FILE = "uostory_cookies.json"

# ì—¬ëŸ¬ ë©”ë‰´ì—ì„œ í”„ë¡œê·¸ë¨ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±°)
PROGRAM_MENUS = [
    {
        "name": "ë¹„êµê³¼ í”„ë¡œê·¸ë¨",
        "menuid": "001003002001",
        "submode": "lecture",
        "reservegroupid": "1",
        "rectype": "L"
    },
    {
        "name": "ì·¨ì—… í”„ë¡œê·¸ë¨",
        "menuid": "001002002002",
        "reservegroupid": "1",
        "rectype": "J"
    }
]

REQUEST_SLEEP_MIN = 5.0  # ìµœì†Œ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
REQUEST_SLEEP_MAX = 10.0  # ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
RECENT_WINDOW_PER_MENU = 50  # ê° ë©”ë‰´ë³„ ìˆ˜ì§‘í•  í”„ë¡œê·¸ë¨ ê°œìˆ˜
MAX_PAGES = 10  # ìµœëŒ€ í˜ì´ì§€ ìˆ˜
CONNECT_TIMEOUT = 10
READ_TIMEOUT = 20

# =========================
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° ì´ˆê¸°í™”
# =========================
from dotenv import load_dotenv
load_dotenv()

# OpenAI API ì´ˆê¸°í™”
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    print(" OPENAI_API_KEY í™•ì¸í•˜ì„¸ìš”.")
    print("OCR í…ìŠ¤íŠ¸ ì •ë¦¬ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    OPENAI_CLIENT = None
else:
    OPENAI_CLIENT = OpenAI(api_key=OPENAI_API_KEY)
    print("OpenAI API ì´ˆê¸°í™” ì™„ë£Œ")

# MySQL DB ì„¤ì •
DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'localhost'),
    'user': os.getenv('DB_USER', 'root'),
    'password': os.getenv('DB_PASSWORD', ''),
    'database': os.getenv('DB_NAME', ''),
    'port': int(os.getenv('DB_PORT', 3306)),
    'charset': os.getenv('DB_CHARSET', 'utf8mb4'),
    'autocommit': os.getenv('DB_AUTOCOMMIT', 'False') == 'True',
    'use_pure': os.getenv('DB_USE_PURE', 'True') == 'True',
}

# =========================
# EasyOCR ì´ˆê¸°í™”
# =========================
# ì „ì—­ìœ¼ë¡œ í•œ ë²ˆë§Œ
log_print = lambda msg: print(f"[{datetime.now():%Y-%m-%d %H:%M:%S}] {msg}")
log_print("EasyOCR ì´ˆê¸°í™” ì¤‘... (ìµœì´ˆ ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)")
OCR_READER = easyocr.Reader(['ko', 'en'], gpu=False, verbose=False)
log_print("EasyOCR ì´ˆê¸°í™” ì™„ë£Œ")

# ì¿ í‚¤ ë¡œë“œ (cookies.json íŒŒì¼ì—ì„œ)
COOKIE_FILE_PATH = "cookies.json"

if not os.path.exists(COOKIE_FILE_PATH):
    print(f"ERROR: ì¿ í‚¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {COOKIE_FILE_PATH}")
    print("cookies.json íŒŒì¼ì„ ìƒì„±í•´ì£¼ì„¸ìš”.")
    exit(1)

try:
    with open(COOKIE_FILE_PATH, 'r', encoding='utf-8') as f:
        RAW_COOKIES = json.load(f)
    print(f"ì¿ í‚¤ ë¡œë“œ ì™„ë£Œ: {len(RAW_COOKIES)}ê°œ")
except json.JSONDecodeError as e:
    print(f"ERROR: ì¿ í‚¤ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
    print(f"{COOKIE_FILE_PATH} íŒŒì¼ì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    exit(1)
except Exception as e:
    print(f"ERROR: ì¿ í‚¤ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    exit(1)

# requestsìš© ì¿ í‚¤ ë”•ì…”ë„ˆë¦¬ ìƒì„±
COOKIES = {cookie['name']: cookie['value'] for cookie in RAW_COOKIES}

# =========================
# ìœ í‹¸ë¦¬í‹°
# =========================

def log(msg: str) -> None:
    print(f"[{datetime.now():%Y-%m-%d %H:%M:%S}] {msg}")


def get_headers() -> dict:
    return {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9",
        "Referer": BASE_URL,
    }


def clean_content(text: str) -> str:
    """ë‚´ìš© ì •ë¦¬: ê³¼ë„í•œ ì¤„ë°”ê¿ˆ ì œê±° ë° ë¬¸ë‹¨ ì •ë¦¬"""
    if not text:
        return ""

    # ì—°ì†ëœ ê³µë°±/íƒ­ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ
    text = re.sub(r'[ \t]+', ' ', text)

    # ì¤„ë°”ê¿ˆ ì •ë¦¬
    # 1. ë‹¨ì¼ ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ (ë¬¸ì¥ ì—°ê²°)
    text = re.sub(r'([^\n])\n([^\n])', r'\1 \2', text)

    # 2. ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ ìµœëŒ€ 2ê°œë¡œ (ë¬¸ë‹¨ êµ¬ë¶„)
    text = re.sub(r'\n{2,}', '\n\n', text)

    # 3. ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()

    return text


def classify_program_categories(title: str, content: str) -> List[str]:
    """í”„ë¡œê·¸ë¨ ì œëª©ê³¼ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜ (ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)"""
    text = (title + " " + content).lower()
    categories = []

    # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ íŒ¨í„´
    patterns = {
        "ë¹„êµê³¼": r"ë¹„êµê³¼",
        "ê³µëª¨ì „": r"ê³µëª¨ì „|ê²½ì§„ëŒ€íšŒ|ëŒ€íšŒ|ì½˜í…ŒìŠ¤íŠ¸|contest|competition",
        "ë©˜í† ë§": r"ë©˜í† ë§|ë©˜í† |ë©˜í‹°|ì½”ì¹­|ìƒë‹´",
        "ë´‰ì‚¬": r"ë´‰ì‚¬|ìì›ë´‰ì‚¬|ì‚¬íšŒê³µí—Œ|volunteer",
        "ì·¨ì—…": r"ì·¨ì—…|ì±„ìš©|ë©´ì ‘|ì´ë ¥ì„œ|ìê¸°ì†Œê°œì„œ|ì»¤ë¦¬ì–´|ì¸í„´|job|career|employment|ì…ì‚¬",
        "íƒë°©": r"íƒë°©|ê²¬í•™|ë°©ë¬¸|íˆ¬ì–´|ë‹µì‚¬|field.?trip",
        "íŠ¹ê°•": r"íŠ¹ê°•|ê°•ì—°|ì„¸ë¯¸ë‚˜|ì›Œí¬ìƒµ|êµìœ¡|lecture|seminar|workshop",
    }

    # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë§¤ì¹­ í™•ì¸ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)
    for category, pattern in patterns.items():
        if re.search(pattern, text):
            categories.append(category)

    log(f"âœ… ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜: {', '.join(categories) if categories else '(ì—†ìŒ)'}")
    return categories


def clean_ocr_text_with_ai(ocr_text: str) -> Optional[str]:
    """OpenAI APIë¡œ OCR í…ìŠ¤íŠ¸ ì •ë¦¬ ë° êµ¬ì¡°í™”"""
    if not OPENAI_CLIENT:
        log("OpenAI API ë¯¸ì„¤ì • - ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜")
        return ocr_text

    try:
        log(f"AIë¡œ í…ìŠ¤íŠ¸ ì •ë¦¬ ì¤‘... ({len(ocr_text)} ê¸€ì)")

        prompt = f"""ë‹¤ìŒì€ í¬ìŠ¤í„°ì—ì„œ OCRë¡œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
ì˜¤íƒ€ì™€ ë„ì–´ì“°ê¸°ë¥¼ ìˆ˜ì •í•˜ê³ , í•µì‹¬ ì •ë³´ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

ê°€ëŠ¥í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±:
- í–‰ì‚¬ëª…/í”„ë¡œê·¸ë¨ëª…: (ìˆìœ¼ë©´ ì‘ì„±)
- ì¼ì‹œ: (ìˆìœ¼ë©´ ì‘ì„±)
- ì¥ì†Œ: (ìˆìœ¼ë©´ ì‘ì„±)
- ê°•ì‚¬/ì£¼ì œ: (ìˆìœ¼ë©´ ì‘ì„±)
- ì‹ ì²­ë°©ë²•: (ìˆìœ¼ë©´ ì‘ì„±)
- ë¬¸ì˜ì²˜: (ìˆìœ¼ë©´ ì‘ì„±)
- ê¸°íƒ€ ë‚´ìš©: (ìœ„ì— í¬í•¨ë˜ì§€ ì•Šì€ ì¤‘ìš” ì •ë³´)

ì—†ëŠ” í•­ëª©ì€ ìƒëµí•˜ê³ , ìˆëŠ” ì •ë³´ë§Œ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.


OCR í…ìŠ¤íŠ¸:
{ocr_text}
"""

        response = OPENAI_CLIENT.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ OCR í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì˜¤íƒ€ë¥¼ ìˆ˜ì •í•˜ê³  ì •ë³´ë¥¼ êµ¬ì¡°í™”í•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )

        cleaned_text = response.choices[0].message.content.strip()
        log(f"AI ì •ë¦¬ ì™„ë£Œ: {len(cleaned_text)} ê¸€ì")
        return cleaned_text

    except Exception as e:
        log(f"AI ì •ë¦¬ ì‹¤íŒ¨: {e}")
        log("ì›ë³¸ OCR í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤")
        return ocr_text


def extract_text_from_image(image_url: str) -> Optional[str]:
    """ì´ë¯¸ì§€ URLì—ì„œ OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    import base64

    try:
        log(f"ì´ë¯¸ì§€ OCR ì‹œì‘: {image_url[:80]}...")

        # base64 ë°ì´í„° URI ì²´í¬
        if image_url.startswith('data:image'):
            # data:image/png;base64,iVBORw0KG... í˜•ì‹ ì²˜ë¦¬
            try:
                header, encoded = image_url.split(',', 1)
                image_data = base64.b64decode(encoded)
                log(f"base64 ë°ì´í„° URI ë””ì½”ë”© ì™„ë£Œ")
            except Exception as e:
                log(f"base64 ë””ì½”ë”© ì‹¤íŒ¨: {e}")
                return None
        else:
            # ì¼ë°˜ URLì—ì„œ ë‹¤ìš´ë¡œë“œ
            # ì™¸ë¶€ ë„ë©”ì¸ì¸ ê²½ìš° Referer ì²˜ë¦¬
            from urllib.parse import urlparse

            parsed_url = urlparse(image_url)
            if parsed_url.netloc and 'uos.ac.kr' not in parsed_url.netloc:
                # ì™¸ë¶€ ë„ë©”ì¸ - Refererë¥¼ í•´ë‹¹ ë„ë©”ì¸ìœ¼ë¡œ ì„¤ì •
                referer = f"{parsed_url.scheme}://{parsed_url.netloc}/"
            else:
                # ë‚´ë¶€ ë„ë©”ì¸ - BASE_URL ì‚¬ìš©
                referer = BASE_URL

            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
                "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
                "Referer": referer,
                "Sec-Fetch-Dest": "image",
                "Sec-Fetch-Mode": "no-cors",
                "Sec-Fetch-Site": "cross-site"
            }

            response = requests.get(
                image_url,
                headers=headers,
                cookies=COOKIES,
                timeout=(CONNECT_TIMEOUT, READ_TIMEOUT)
            )

            if response.status_code != 200:
                log(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: HTTP {response.status_code}")
                return None

            image_data = response.content

        # PIL Imageë¡œ ë³€í™˜
        image = Image.open(BytesIO(image_data))

        # RGBë¡œ ë³€í™˜ (RGBA ë“±ì˜ ê²½ìš° ëŒ€ë¹„)
        if image.mode != 'RGB':
            image = image.convert('RGB')

        # numpy arrayë¡œ ë³€í™˜
        image_np = np.array(image)

        # OCR ì‹¤í–‰
        log(f"OCR ì‹¤í–‰ ì¤‘...")
        results = OCR_READER.readtext(image_np)

        # ê²°ê³¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì‹ ë¢°ë„ 0.3 ì´ìƒë§Œ)
        extracted_lines = []
        for (bbox, text, confidence) in results:
            if confidence > 0.3:
                extracted_lines.append(text)

        extracted_text = '\n'.join(extracted_lines)

        log(f"OCR ì™„ë£Œ: {len(extracted_lines)}ê°œ í…ìŠ¤íŠ¸ ë¼ì¸ ì¶”ì¶œ")
        return extracted_text.strip()

    except Exception as e:
        log(f"OCR ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None


def parse_departments(dept_text: str) -> list:
    """í•™ê³¼ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ í•™ê³¼ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    if not dept_text:
        return ['ì œí•œì—†ìŒ']

    # "í•™ë…„" í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í•™ê³¼ ë¶€ë¶„ë§Œ ì¶”ì¶œ
    # ì˜ˆ: "ì œí•œì—†ìŒí•™ë…„ : ëŒ€í•™ì›ìƒ" â†’ "ì œí•œì—†ìŒ"ë§Œ ì¶”ì¶œ
    dept_only = re.split(r'í•™ë…„', dept_text)[0].strip()

    # ì‰¼í‘œë‚˜ / ë¡œ êµ¬ë¶„ëœ í•™ê³¼ë“¤ì„ ë¶„ë¦¬
    departments = re.split(r'[,/]', dept_only)

    result = []
    for dept in departments:
        dept = dept.strip()
        # ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œ ì œê±°
        dept = re.sub(r'[:ï¼š\s]+$', '', dept)  # ëì˜ ì½œë¡ , ê³µë°± ì œê±°

        if dept and dept not in ['ì œí•œì—†ìŒ', '']:
            result.append(dept)

    return result if result else ['ì œí•œì—†ìŒ']


def parse_grades(grade_text: str) -> list:
    """í•™ë…„ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ í•™ë…„ ì½”ë“œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    ë§¤í•‘:
    - 0: ì œí•œì—†ìŒ/ì „ì²´
    - 1~5: 1~5í•™ë…„
    - 6: ì¡¸ì—…ìƒ
    - 7: ëŒ€í•™ì›ìƒ
    """
    if not grade_text:
        return [0]

    grades = []

    # ìˆ«ì í•™ë…„ ì¶”ì¶œ (1í•™ë…„, 2í•™ë…„ ë“±)
    numeric_grades = re.findall(r'(\d+)í•™ë…„', grade_text)
    for g in numeric_grades:
        grade_num = int(g)
        if 1 <= grade_num <= 5:
            grades.append(grade_num)

    # ì¡¸ì—…ìƒ ì²´í¬
    if re.search(r'ì¡¸ì—…ìƒ?', grade_text):
        grades.append(6)

    # ëŒ€í•™ì›ìƒ ì²´í¬
    if re.search(r'ëŒ€í•™ì›ìƒ?', grade_text):
        grades.append(7)

    # ì œí•œì—†ìŒ/ì „ì²´ ì²´í¬
    if re.search(r'ì œí•œ\s*ì—†ìŒ|ì „ì²´', grade_text) or not grades:
        return [0]

    return grades



def generate_mysql_json_object(data: dict) -> str:
    """ë°ì´í„°ë¥¼ MySQL JSON_OBJECT í˜•ì‹ì˜ SET ë¬¸ìœ¼ë¡œ ë³€í™˜"""

    # ë¬¸ìì—´ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬ (ì‘ì€ë”°ì˜´í‘œë¥¼ ë‘ ê°œë¡œ)
    def escape_sql_string(s):
        if s is None:
            return 'NULL'
        return s.replace("'", "''").replace('\\', '\\\\')

    # JSON_ARRAY ìƒì„± í•¨ìˆ˜
    def to_json_array(arr, is_numeric=False):
        if not arr:
            return "JSON_ARRAY()"
        if is_numeric:
            # ìˆ«ì ë°°ì—´
            items = ','.join(str(x) for x in arr)
        else:
            # ë¬¸ìì—´ ë°°ì—´
            items = ','.join(f"'{escape_sql_string(x)}'" for x in arr)
        return f"JSON_ARRAY({items})"

    # ê° í•„ë“œ ì¶”ì¶œ
    title = escape_sql_string(data.get('title', ''))
    category_array = to_json_array(data.get('categories', []))
    link = escape_sql_string(data.get('link', ''))
    content = escape_sql_string(data.get('content', ''))
    app_start_date = data.get('app_start_date') or 'NULL'
    app_end_date = data.get('app_end_date') or 'NULL'
    departments_array = to_json_array(data.get('department', []))
    grades_array = to_json_array(data.get('grade', []), is_numeric=True)

    # SET ë¬¸ ìƒì„±
    sql = f"""SET @p = JSON_OBJECT(
  'title', '{title}',
  'category', {category_array},
  'link', '{link}',
  'content', '{content}',
  'app_start_date', {'NULL' if app_start_date == 'NULL' else "'" + app_start_date + "'"},
  'app_end_date', {'NULL' if app_end_date == 'NULL' else "'" + app_end_date + "'"},
  'departments', {departments_array},
  'grades', {grades_array}
);"""

    return sql


def get_db_connection():
    """MySQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒì„±"""
    try:
        connection = mysql.connector.connect(**DB_CONFIG)
        if connection.is_connected():
            log("MySQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
            return connection
    except Error as e:
        log(f"MySQL ì—°ê²° ì‹¤íŒ¨: {e}")
        return None


def insert_program_to_db(data: dict) -> str:
    """
    í”„ë¡œê·¸ë¨ ë°ì´í„°ë¥¼ DBì˜ program í…Œì´ë¸”ì— ì‚½ì…
    Returns: 'success', 'duplicate', 'error'
    """
    connection = None
    cursor = None

    try:
        connection = get_db_connection()
        if not connection:
            return 'error'

        cursor = connection.cursor()

        # [ê°±ì‹  ëª¨ë“œ] ì¤‘ë³µ ì‹œ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ì¬ì‚½ì…
        link = data.get('link', '')
        if link:
            check_query = "SELECT id FROM program WHERE link = %s LIMIT 1"
            cursor.execute(check_query, (link,))
            existing = cursor.fetchone()

            if existing:
                existing_id = existing[0]
                log(f"ğŸ”„ ê¸°ì¡´ ë°ì´í„° ë°œê²¬ (ID: {existing_id}) - ì‚­ì œ í›„ ì¬ì‚½ì…")

                # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (program_categoryëŠ” ON DELETE CASCADEë¡œ ìë™ ì‚­ì œë¨)
                cursor.execute("DELETE FROM program WHERE id = %s", (existing_id,))
                connection.commit()
                log(f"  âœ… ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")

        # í•™ê³¼ ë° í•™ë…„ íŒŒì‹±
        departments = parse_departments(data.get('target_department', ''))
        grades = parse_grades(data.get('target_grade', ''))
        categories = data.get('categories', [])

        # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€í•˜ë©´ì„œ)
        departments = list(dict.fromkeys(departments))  # ì¤‘ë³µ ì œê±°
        grades = list(dict.fromkeys(grades))
        categories = list(dict.fromkeys(categories))

        # Stored Procedureì— ì „ë‹¬í•  JSON ê°ì²´ ìƒì„±
        program_data = {
            'title': data.get('title', ''),
            'link': data.get('link', ''),
            'content': data.get('content', ''),
            'categories': categories,
            'departments': departments,
            'grades': grades
        }

        # ë‚ ì§œ í•„ë“œê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if data.get('application_start'):
            program_data['app_start_date'] = data.get('application_start')
        if data.get('application_end'):
            program_data['app_end_date'] = data.get('application_end')

        # JSON ë¬¸ìì—´ë¡œ ë³€í™˜
        json_data = json.dumps(program_data, ensure_ascii=False)

        # Stored Procedure í˜¸ì¶œ (OUT íŒŒë¼ë¯¸í„°)
        args = [json_data, 0]
        result_args = cursor.callproc('sp_create_program', args)
        connection.commit()

        # OUT íŒŒë¼ë¯¸í„°ì—ì„œ program_id ê°€ì ¸ì˜¤ê¸°
        program_id = result_args[1]

        log(f"DB ì‚½ì… ì„±ê³µ: {data.get('title', '')[:30]}... (ID: {program_id})")
        return 'success'

    except Error as e:
        log(f"DB ì‚½ì… ì‹¤íŒ¨: {e}")
        if connection:
            connection.rollback()
        return 'error'

    finally:
        if cursor:
            cursor.close()
        if connection and connection.is_connected():
            connection.close()


def print_program_info(data: dict) -> None:
    print("\n" + "="*80)
    print(f"í”„ë¡œê·¸ë¨ ID: {data['program_id']}")
    print(f"ì œëª©: {data['title']}")
    print(f"ë§í¬: {data.get('link', '')}")
    print("-"*80)

    if data.get('categories'):
        print(f"ì¹´í…Œê³ ë¦¬: {', '.join(data['categories'])}")
    elif data.get('category'):
        print(f"ì¹´í…Œê³ ë¦¬: {data['category']}")
    if data.get('target_department'):
        print(f"ëŒ€ìƒí•™ê³¼: {data['target_department']}")
    if data.get('target_grade'):
        print(f"ëŒ€ìƒí•™ë…„: {data['target_grade']}")
    if data.get('selection_method'):
        print(f"ì„ ë°œë°©ì‹: {data['selection_method']}")
    if data.get('capacity'):
        print(f"ëª¨ì§‘ì¸ì›: {data['capacity']}ëª…")
    if data.get('location'):
        print(f"ì¥ì†Œ: {data['location']}")

    print("-"*80)

    if data.get('application_start'):
        print(f"ì‹ ì²­ ì‹œì‘: {data['application_start']}")
    if data.get('application_end'):
        print(f"ì‹ ì²­ ë§ˆê°: {data['application_end']}")
    if data.get('operation_start'):
        print(f"ìš´ì˜ ì‹œì‘: {data['operation_start']}")
    if data.get('operation_end'):
        print(f"ìš´ì˜ ë§ˆê°: {data['operation_end']}")
    if data.get('status'):
        print(f"ìƒíƒœ: {data['status']}")

    print("-"*80)

    if data.get('content'):
        cleaned = clean_content(data['content'])
        print(f"ë‚´ìš©:\n{cleaned}")

    print("="*80 + "\n")


# =========================
# ëª©ë¡ ìˆ˜ì§‘
# =========================

def extract_program_ids_from_html(html: str) -> List[int]:
    soup = BeautifulSoup(html, "html.parser")
    program_ids = []

    for link in soup.find_all("a", href=True):
        href = link.get("href", "")
        if "lectureDetail" in href and "lecturegroupid" in href:
            match = re.search(r"lecturegroupid[=](\d+)", href)
            if match:
                program_ids.append(int(match.group(1)))

    seen = set()
    result = []
    for pid in program_ids:
        if pid not in seen:
            seen.add(pid)
            result.append(pid)

    return result


def collect_program_ids(limit_per_menu: int = RECENT_WINDOW_PER_MENU, max_pages: int = MAX_PAGES) -> List[int]:
    """ì—¬ëŸ¬ ë©”ë‰´ì—ì„œ í”„ë¡œê·¸ë¨ ID ìˆ˜ì§‘ (ê° ë©”ë‰´ë³„ë¡œ limit_per_menuê°œì”©)"""
    all_program_ids = []
    global_seen = set()  # ì „ì—­ ì¤‘ë³µ ì²´í¬

    # ê° ë©”ë‰´ë³„ë¡œ í¬ë¡¤ë§
    for menu in PROGRAM_MENUS:
        log(f"\n{'='*60}")
        log(f"[{menu['name']}] ë©”ë‰´ í¬ë¡¤ë§ ì‹œì‘ (ëª©í‘œ: {limit_per_menu}ê°œ)")
        log(f"{'='*60}")

        menu_program_ids = []  # ì´ ë©”ë‰´ì—ì„œ ìˆ˜ì§‘í•œ IDë“¤

        for page in range(1, max_pages + 1):
            try:
                log(f"[{menu['name']}] í˜ì´ì§€ {page} ìš”ì²­ ì¤‘...")

                # í˜ì´ì§€ë³„ íŒŒë¼ë¯¸í„°
                params = menu.copy()
                params.pop('name')  # nameì€ íŒŒë¼ë¯¸í„°ê°€ ì•„ë‹˜
                params["currentpage"] = str(page)
                params["viewtype"] = "L"
                params["thumbnail"] = "Y"

                response = requests.get(
                    LIST_URL,
                    params=params,
                    headers=get_headers(),
                    cookies=COOKIES,
                    timeout=(CONNECT_TIMEOUT, READ_TIMEOUT),
                    allow_redirects=True
                )

                if response.status_code != 200:
                    log(f"í˜ì´ì§€ {page} HTTP {response.status_code}")
                    break

                if "login" in response.url.lower() or "sso" in response.url.lower():
                    log("ë¡œê·¸ì¸ í•„ìš” - ì¿ í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”")
                    break

                page_program_ids = extract_program_ids_from_html(response.text)

                # ì´ ë©”ë‰´ì—ì„œ ì•„ì§ ì•ˆ ë³¸ í”„ë¡œê·¸ë¨ë§Œ ì¶”ê°€
                new_count = 0
                duplicate_count = 0
                for pid in page_program_ids:
                    if pid in global_seen:
                        duplicate_count += 1  # ë‹¤ë¥¸ ë©”ë‰´ì—ì„œ ì´ë¯¸ ìˆ˜ì§‘í•¨
                    elif len(menu_program_ids) < limit_per_menu:
                        menu_program_ids.append(pid)
                        global_seen.add(pid)
                        new_count += 1

                log(f"[{menu['name']}] í˜ì´ì§€ {page}: {len(page_program_ids)}ê°œ ë°œê²¬, "
                    f"{new_count}ê°œ ì‹ ê·œ, {duplicate_count}ê°œ ì¤‘ë³µ "
                    f"(ë©”ë‰´ ëˆ„ì : {len(menu_program_ids)}/{limit_per_menu})")

                # ì´ ë©”ë‰´ì—ì„œ ëª©í‘œ ê°œìˆ˜ ë„ë‹¬
                if len(menu_program_ids) >= limit_per_menu:
                    log(f"[{menu['name']}] ëª©í‘œ {limit_per_menu}ê°œ ë‹¬ì„± - ë‹¤ìŒ ë©”ë‰´ë¡œ")
                    break

                # ë‹¤ìŒ í˜ì´ì§€ ìš”ì²­ ì „ ëŒ€ê¸°
                if page < max_pages:
                    time.sleep(2)

            except Exception as e:
                log(f"í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                break

        # ì´ ë©”ë‰´ì˜ ê²°ê³¼ë¥¼ ì „ì²´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        all_program_ids.extend(menu_program_ids)
        log(f"[{menu['name']}] ìµœì¢…: {len(menu_program_ids)}ê°œ ìˆ˜ì§‘")

    log(f"\n{'='*60}")
    log(f"ì „ì²´ ë©”ë‰´ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_program_ids)}ê°œ í”„ë¡œê·¸ë¨ ë°œê²¬")
    log(f"{'='*60}\n")
    return all_program_ids


# =========================
# ìƒì„¸ í˜ì´ì§€ íŒŒì‹±
# =========================

def parse_date_range(text: str) -> tuple[Optional[str], Optional[str]]:
    """ë‚ ì§œ ë²”ìœ„ íŒŒì‹± (ì‹œê°„ í¬í•¨ ê°€ëŠ¥)

    ì˜ˆ: "2025-11-07 10:00:00 ~ 2025-11-14 23:59:00" â†’ ("2025-11-07", "2025-11-14")
    """
    if not text:
        return None, None

    # íŒ¨í„´ 1: ë‚ ì§œ+ì‹œê°„ ë²”ìœ„ (YYYY-MM-DD HH:MM:SS ~ YYYY-MM-DD HH:MM:SS)
    match = re.search(r'(\d{4}-\d{2}-\d{2})\s+\d{2}:\d{2}:\d{2}\s*~\s*(\d{4}-\d{2}-\d{2})\s+\d{2}:\d{2}:\d{2}', text)
    if match:
        return match.group(1), match.group(2)

    # íŒ¨í„´ 2: ë‚ ì§œë§Œ ë²”ìœ„ (YYYY-MM-DD ~ YYYY-MM-DD)
    match = re.search(r'(\d{4}-\d{2}-\d{2})\s*~\s*(\d{4}-\d{2}-\d{2})', text)
    if match:
        return match.group(1), match.group(2)

    # íŒ¨í„´ 3: ë‹¨ì¼ ë‚ ì§œ+ì‹œê°„ (YYYY-MM-DD HH:MM:SS)
    match = re.search(r'(\d{4}-\d{2}-\d{2})\s+\d{2}:\d{2}:\d{2}', text)
    if match:
        return match.group(1), match.group(1)

    # íŒ¨í„´ 4: ë‹¨ì¼ ë‚ ì§œ (YYYY-MM-DD)
    match = re.search(r'(\d{4}-\d{2}-\d{2})', text)
    if match:
        return match.group(1), match.group(1)

    return None, None


def fetch_program_html_with_playwright(program_id: int) -> Optional[str]:
    """Playwrightë¡œ ìë°”ìŠ¤í¬ë¦½íŠ¸ ë Œë”ë§ í›„ HTML ê°€ì ¸ì˜¤ê¸°"""
    try:
        params = {
            "menuid": "001003002001",
            "reservegroupid": "1",
            "viewtype": "L",
            "rectype": "L",
            "thumbnail": "Y",
            "lecturegroupid": str(program_id)
        }

        url = f"{DETAIL_URL}?{urlencode(params)}"
        log(f"í”„ë¡œê·¸ë¨ {program_id} Playwrightë¡œ ìš”ì²­ ì¤‘...")

        with sync_playwright() as p:
            # ë¸Œë¼ìš°ì € ì˜µì…˜ ì„¤ì • (ë´‡ ê°ì§€ ìš°íšŒ)
            browser = p.chromium.launch(
                headless=True,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-dev-shm-usage',
                    '--no-sandbox'
                ]
            )

            # ì»¨í…ìŠ¤íŠ¸ ì„¤ì • (ì¼ë°˜ ë¸Œë¼ìš°ì €ì²˜ëŸ¼)
            context = browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080},
                locale='ko-KR',
                timezone_id='Asia/Seoul'
            )

            # ì›ë³¸ ì¿ í‚¤ ê·¸ëŒ€ë¡œ ì‚¬ìš© (domain, path, expires ë“± ëª¨ë‘ í¬í•¨)
            playwright_cookies = []
            for cookie in RAW_COOKIES:
                playwright_cookie = {
                    "name": cookie['name'],
                    "value": cookie['value'],
                    "domain": cookie.get('domain', '.uos.ac.kr'),
                    "path": cookie.get('path', '/'),
                }

                # expires ìˆìœ¼ë©´ ì¶”ê°€
                if 'expires' in cookie and cookie['expires'] != -1:
                    playwright_cookie['expires'] = cookie['expires']

                # httpOnly, secure í”Œë˜ê·¸
                if 'httpOnly' in cookie:
                    playwright_cookie['httpOnly'] = cookie['httpOnly']
                if 'secure' in cookie:
                    playwright_cookie['secure'] = cookie['secure']

                # sameSite
                if 'sameSite' in cookie:
                    playwright_cookie['sameSite'] = cookie['sameSite']

                playwright_cookies.append(playwright_cookie)

            context.add_cookies(playwright_cookies)
            log(f"{len(playwright_cookies)}ê°œ ì¿ í‚¤ ì„¤ì • ì™„ë£Œ")

            page = context.new_page()

            # í˜ì´ì§€ ë¡œë“œ (ë” ê¸´ íƒ€ì„ì•„ì›ƒ)
            try:
                page.goto(url, wait_until="domcontentloaded", timeout=30000)

                # ì¶”ê°€ ëŒ€ê¸° (ìë°”ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ëŒ€ê¸°)
                page.wait_for_timeout(3000)

                # ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ ë˜ì—ˆëŠ”ì§€ í™•ì¸
                current_url = page.url
                if "login" in current_url.lower() or "sso" in current_url.lower():
                    log(f"ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ë¨: {current_url}")
                    log("ì¿ í‚¤ê°€ ë§Œë£Œë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. uostory_login.pyë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
                    browser.close()
                    return None

            except Exception as e:
                log(f"âš ï¸ í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ: {e}")

            # HTML ê°€ì ¸ì˜¤ê¸°
            html = page.content()

            browser.close()

        log(f"í”„ë¡œê·¸ë¨ {program_id} ë¡œë“œ ì„±ê³µ")
        return html

    except Exception as e:
        log(f"ìš”ì²­ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None


def parse_program_fields(html: str, program_id: int) -> Optional[dict]:
    soup = BeautifulSoup(html, "html.parser")

    # ì œëª© ì¶”ì¶œ (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
    title = ""

    # íŒ¨í„´ 1: id="lecturetitle" (ìƒì„¸ í˜ì´ì§€)
    title_el = soup.select_one("#lecturetitle")
    if title_el:
        title = title_el.get_text(strip=True)

    # íŒ¨í„´ 2: page_title í´ë˜ìŠ¤
    if not title:
        title_el = soup.select_one("h2.page_title") or soup.select_one("h3")
        if title_el:
            title = title_el.get_text(strip=True)

    # íŒ¨í„´ 3: í…Œì´ë¸”ì—ì„œ "ê³¼ì •ëª…" ì°¾ê¸°
    if not title:
        for tr in soup.select("tr"):
            th = tr.select_one("th")
            if th and "ê³¼ì •ëª…" in th.get_text(strip=True):
                td = tr.select_one("td")
                if td:
                    link = td.select_one("a")
                    title = link.get_text(strip=True) if link else td.get_text(strip=True)
                    break

    if not title:
        log(f"Program {program_id}: ì œëª© ì—†ìŒ")
        return None

    fields = {}

    # trans_thead ìŠ¤íƒ€ì¼
    for tr in soup.select("tr.trans_thead"):
        th = tr.select_one("th")
        td = tr.select_one("td")
        if not th or not td:
            continue

        th_text = th.get_text(strip=True)
        td_text = td.get_text(strip=True)

        if "ëŒ€ìƒ" in th_text:
            dept_match = re.search(r'í•™ê³¼\s*[:ï¼š]\s*([^\n]+)', td_text)
            grade_match = re.search(r'í•™ë…„\s*[:ï¼š]\s*([^\n]+)', td_text)
            fields["target_department"] = dept_match.group(1).strip() if dept_match else None
            fields["target_grade"] = grade_match.group(1).strip() if grade_match else None
        elif "ì„ ë°œë°©ì‹" in th_text or "ì„ ë°œ" in th_text:
            fields["selection_method"] = td_text
        elif "ëª¨ì§‘ì¸ì›" in th_text or "ì¸ì›" in th_text:
            num_match = re.search(r'(\d+)', td_text)
            fields["capacity"] = int(num_match.group(1)) if num_match else None
        elif "ì¥ì†Œ" in th_text:
            fields["location"] = td_text
        elif "ì¹´í…Œê³ ë¦¬" in th_text or "í•µì‹¬ì—­ëŸ‰" in th_text:
            fields["category"] = td_text

    # ì¼ë°˜ í…Œì´ë¸”
    for tr in soup.select("tr"):
        th = tr.select_one("th")
        td = tr.select_one("td")
        if not th or not td:
            continue

        th_text = th.get_text(strip=True)
        td_text = td.get_text(strip=True)

        if "ì‹ ì²­ê¸°ê°„" in th_text:
            start, end = parse_date_range(td_text)
            fields["application_start"] = start
            fields["application_end"] = end
        elif "ìš´ì˜ê¸°ê°„" in th_text:
            start, end = parse_date_range(td_text)
            fields["operation_start"] = start
            fields["operation_end"] = end
        elif "ëŒ€ìƒí•™ê³¼" in th_text and "target_department" not in fields:
            fields["target_department"] = td_text
        elif "ëŒ€ìƒí•™ë…„" in th_text and "target_grade" not in fields:
            fields["target_grade"] = td_text

    # ë³¸ë¬¸ ì°¾ê¸°: "ìƒì„¸ë‚´ìš©" th ë‹¤ìŒì˜ td
    content_el = None
    content = ""

    # "ìƒì„¸ë‚´ìš©" thë¥¼ ì°¾ê³  ê·¸ ë‹¤ìŒ trì˜ tdë¥¼ ê°€ì ¸ì˜¤ê¸°
    for th in soup.find_all('th'):
        if 'ìƒì„¸ë‚´ìš©' in th.get_text(strip=True):
            # ìƒì„¸ë‚´ìš© thê°€ ìˆëŠ” trì˜ ë‹¤ìŒ tr ì°¾ê¸°
            parent_tr = th.find_parent('tr')
            if parent_tr:
                next_tr = parent_tr.find_next_sibling('tr')
                if next_tr:
                    content_el = next_tr.find('td')
                    break

    # í´ë°±: "ìƒì„¸ë‚´ìš©"ì„ ëª» ì°¾ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹
    if not content_el:
        content_el = soup.select_one("td[colspan='10']") or soup.select_one("tbody td")

    if content_el:
        content = content_el.get_text("\n", strip=True)

    # ë‚´ìš© ì •ë¦¬
    content = clean_content(content)

    # ë³¸ë¬¸ ë‚´ ì´ë¯¸ì§€ ì°¾ê¸° ë° OCR ì‹¤í–‰
    image_urls = []

    if content_el:
        # "ìƒì„¸ë‚´ìš©" ì•„ë˜ td ì•ˆì˜ ëª¨ë“  ì´ë¯¸ì§€ ì°¾ê¸°
        for img in content_el.find_all('img'):
            img_src = img.get('src', '')
            if not img_src:
                continue

            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if img_src.startswith('/'):
                img_url = BASE_URL + img_src
            elif img_src.startswith('http'):
                img_url = img_src
            else:
                img_url = BASE_URL + '/' + img_src

            # ì¤‘ë³µ ì œê±°
            if img_url not in image_urls:
                image_urls.append(img_url)

    if image_urls:
        log(f"ë³¸ë¬¸ ë‚´ ì´ë¯¸ì§€ {len(image_urls)}ê°œ ë°œê²¬")

    # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ OCR ì‹¤í–‰ + AI ì •ë¦¬
    ocr_texts = []
    for idx, img_url in enumerate(image_urls, 1):
        log(f"  [{idx}/{len(image_urls)}] ì´ë¯¸ì§€ OCR ì²˜ë¦¬ ì¤‘...")
        ocr_text = extract_text_from_image(img_url)
        if ocr_text:
            # OpenAI APIë¡œ í…ìŠ¤íŠ¸ ì •ë¦¬
            cleaned_text = clean_ocr_text_with_ai(ocr_text)
            if cleaned_text:
                ocr_texts.append(f"[ì´ë¯¸ì§€ {idx} ì •ë³´]\n{cleaned_text}")

    # OCR ê²°ê³¼ë¥¼ ë³¸ë¬¸ì— ì¶”ê°€
    if ocr_texts:
        log(f"{len(ocr_texts)}ê°œ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ë¦¬ ì™„ë£Œ")
        content = content + "\n\n" + "\n\n".join(ocr_texts)
        content = clean_content(content)
    elif image_urls:
        log(f"ì´ë¯¸ì§€ëŠ” ìˆì§€ë§Œ OCRë¡œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")

    # ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
    categories = classify_program_categories(title, content)

    # ìƒíƒœ
    status = "ëª¨ì§‘ì¤‘"
    if fields.get("application_end"):
        try:
            end_date = datetime.strptime(fields["application_end"], "%Y-%m-%d")
            if end_date < datetime.now():
                status = "ë§ˆê°"
        except:
            pass

    return {
        "program_id": program_id,
        "title": title,
        "categories": categories,  # ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ (ë¦¬ìŠ¤íŠ¸)
        "content": content,
        "target_department": fields.get("target_department"),
        "target_grade": fields.get("target_grade"),
        "selection_method": fields.get("selection_method"),
        "capacity": fields.get("capacity"),
        "location": fields.get("location"),
        "application_start": fields.get("application_start"),
        "application_end": fields.get("application_end"),
        "operation_start": fields.get("operation_start"),
        "operation_end": fields.get("operation_end"),
        "status": status,
        "posted_date": datetime.now().strftime("%Y-%m-%d"),
    }


def process_one_program(program_id: int) -> Optional[dict]:
    html = fetch_program_html_with_playwright(program_id)
    if not html:
        return None

    parsed = parse_program_fields(html, program_id)
    if not parsed:
        return None

    params = {
        "menuid": "001003002001",
        "reservegroupid": "1",
        "viewtype": "L",
        "rectype": "L",
        "thumbnail": "Y",
        "lecturegroupid": str(program_id)
    }
    parsed["link"] = f"{DETAIL_URL}?{urlencode(params)}"

    log(f"íŒŒì‹± ì™„ë£Œ: {parsed['title'][:30]}...")
    return parsed


# =========================
# ë©”ì¸
# =========================

def main():
    log("UOS Story ë¹„êµê³¼ í”„ë¡œê·¸ë¨ í¬ë¡¤ëŸ¬")
    print()

    program_ids = collect_program_ids(limit_per_menu=RECENT_WINDOW_PER_MENU)

    if not program_ids:
        log("í”„ë¡œê·¸ë¨ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
        return 1

    log(f"{len(program_ids)}ê°œ í”„ë¡œê·¸ë¨ ì²˜ë¦¬")
    log(f"ID: {program_ids}")
    print()

    collected = []
    inserted_count = 0
    duplicate_count = 0
    merged_count = 0
    error_count = 0

    for idx, pid in enumerate(program_ids, 1):
        log(f"[{idx}/{len(program_ids)}] í”„ë¡œê·¸ë¨ {pid} ì²˜ë¦¬ ì¤‘...")

        # ë§í¬ ë¯¸ë¦¬ ìƒì„± (DB ì²´í¬ìš©)
        params = {
            "menuid": "001003002001",
            "reservegroupid": "1",
            "viewtype": "L",
            "rectype": "L",
            "thumbnail": "Y",
            "lecturegroupid": str(pid)
        }
        link = f"{DETAIL_URL}?{urlencode(params)}"

        # [ê°±ì‹  ëª¨ë“œ] í¬ë¡¤ë§ ì „ ì¤‘ë³µ ì²´í¬ ë¹„í™œì„±í™” - ëª¨ë“  í”„ë¡œê·¸ë¨ í¬ë¡¤ë§
        # connection = get_db_connection()
        # if connection:
        #     try:
        #         cursor = connection.cursor()
        #         check_query = "SELECT id FROM program WHERE link = %s LIMIT 1"
        #         cursor.execute(check_query, (link,))
        #         existing = cursor.fetchone()
        #
        #         if existing:
        #             log(f"  â­ DBì— ì´ë¯¸ ì¡´ì¬ (ID: {existing[0]}) - í¬ë¡¤ë§ ê±´ë„ˆë›°ê¸°")
        #             duplicate_count += 1
        #             cursor.close()
        #             connection.close()
        #             continue  # ë‹¤ìŒ í”„ë¡œê·¸ë¨ìœ¼ë¡œ
        #
        #         cursor.close()
        #         connection.close()
        #
        #     except Error as e:
        #         log(f"  âš ï¸ DB ì²´í¬ ì‹¤íŒ¨: {e}")
        #         if connection:
        #             connection.close()

        # ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ (DBì— ì—†ëŠ” ê²ƒë§Œ)
        data = process_one_program(pid)
        if data:
            collected.append(data)
            # ìƒì„¸ ì •ë³´ ì¶œë ¥
            print_program_info(data)
            # DB ì €ì¥ìš© ë°ì´í„° ì¶œë ¥

            # DBì— ì‚½ì…
            result = insert_program_to_db(data)
            if result == 'success':
                inserted_count += 1
            elif result == 'merged':
                merged_count += 1
            elif result == 'duplicate':
                duplicate_count += 1
            elif result == 'error':
                error_count += 1
        else:
            error_count += 1

        if idx < len(program_ids):
            sleep_time = random.uniform(REQUEST_SLEEP_MIN, REQUEST_SLEEP_MAX)
            log(f"{sleep_time:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
            time.sleep(sleep_time)

    log(f"\n{'='*60}")
    log(f"í¬ë¡¤ë§ ì™„ë£Œ í†µê³„:")
    log(f"  - ì´ ì²˜ë¦¬: {len(program_ids)}ê°œ")
    log(f"  - ìˆ˜ì§‘ ì„±ê³µ: {len(collected)}ê°œ")
    log(f"  - DB ì‚½ì…: {inserted_count}ê°œ")
    log(f"  - ì¹´í…Œê³ ë¦¬ ë³‘í•©: {merged_count}ê°œ")
    log(f"  - ì¤‘ë³µ ê±´ë„ˆëœ€: {duplicate_count}ê°œ")
    if error_count > 0:
        log(f"  - ì²˜ë¦¬ ì‹¤íŒ¨: {error_count}ê°œ")
    log(f"{'='*60}")

    output_file = "uostory_programs.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(collected, f, ensure_ascii=False, indent=2)
    log(f"{output_file}ì— ì €ì¥")

    log("ì™„ë£Œ!")
    return 0


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        log("\nì¤‘ë‹¨ë¨")
        exit(0)
    except Exception as e:
        log(f"ERROR: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
