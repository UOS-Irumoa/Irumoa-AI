# portal_search_crawler.py
"""
ì„œìš¸ì‹œë¦½ëŒ€ í¬í„¸ ê³µì§€ì‚¬í•­ ê²€ìƒ‰ ê¸°ë°˜ í¬ë¡¤ëŸ¬
- ì¼ë°˜ê³µì§€(FA1)ì—ì„œ ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ì–´ë¡œ í¬ë¡¤ë§
- ê° ê²€ìƒ‰ì–´(ê³µëª¨ì „, íŠ¹ê°•, ë´‰ì‚¬ ë“±)ë³„ë¡œ 50ê°œì”© ìˆ˜ì§‘
- link ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬ (ì œëª© ê¸°ì¤€ ë³‘í•©ì€ ë³„ë„ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ)
"""

import time
import re
import json
import random
import os
from typing import List, Optional, Dict
from datetime import datetime

import requests
from bs4 import BeautifulSoup
import mysql.connector
from mysql.connector import Error
from io import BytesIO
from PIL import Image
import numpy as np
import easyocr
from playwright.sync_api import sync_playwright

# =========================
# ì„¤ì •
# =========================

BASE_URL = "https://www.uos.ac.kr"
LIST_URL = f"{BASE_URL}/korNotice/list.do"
VIEW_URL = f"{BASE_URL}/korNotice/view.do"

# ê²€ìƒ‰ ì¹´í…Œê³ ë¦¬ ì •ì˜ (ê²€ìƒ‰ì–´ = ì¹´í…Œê³ ë¦¬)
SEARCH_CATEGORIES = [
    # {"name": "ê³µëª¨ì „", "keyword": "ê³µëª¨"},
    # {"name": "íŠ¹ê°•", "keyword": "íŠ¹ê°•"},
    # {"name": "ë´‰ì‚¬", "keyword": "ë´‰ì‚¬"},
    # {"name": "ì·¨ì—…", "keyword": "ì·¨ì—…"},
    # {"name": "íƒë°©", "keyword": "íƒë°©"},
    # {"name": "ë©˜í† ë§", "keyword": "ë©˜í† ë§"},
    {"name": "ë¹„êµê³¼", "keyword": "ë¹„êµê³¼"}
]

REQUEST_SLEEP_MIN = 4.0  # ìµœì†Œ ëŒ€ê¸° ì‹œê°„ (ì´ˆ) - ì¦ê°€
REQUEST_SLEEP_MAX = 7.0  # ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ) - ì¦ê°€
NOTICES_PER_CATEGORY = 25  # ê° ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘í•  ê³µì§€ ê°œìˆ˜
MAX_PAGES = 10  # ìµœëŒ€ í˜ì´ì§€ ìˆ˜
CONNECT_TIMEOUT = 10
READ_TIMEOUT = 20

# í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ëœë¤ ìƒ˜í”Œë§í•  ê°œìˆ˜ (Noneì´ë©´ ì „ì²´ í¬ë¡¤ë§)
# TEST_RANDOM_SAMPLE = 3  # í…ŒìŠ¤íŠ¸ìš©: ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ëœë¤ 3ê°œë§Œ
TEST_RANDOM_SAMPLE = None  # ì‹¤ì œ ìš´ì˜: ì „ì²´ í¬ë¡¤ë§

# =========================
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# =========================
from dotenv import load_dotenv
load_dotenv()

# OpenAI API ì´ˆê¸°í™”
from openai import OpenAI

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    print(" OPENAI_API_KEY í™•ì¸í•˜ì„¸ìš”.")
    print("LLM ì •ë³´ ì¶”ì¶œ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    OPENAI_CLIENT = None
else:
    OPENAI_CLIENT = OpenAI(api_key=OPENAI_API_KEY)
    print(" OpenAI API ì´ˆê¸°í™” ì™„ë£Œ")

# =========================
# EasyOCR ì´ˆê¸°í™”
# =========================
log_print = lambda msg: print(f"[{datetime.now():%Y-%m-%d %H:%M:%S}] {msg}")
log_print("EasyOCR ì´ˆê¸°í™” ì¤‘... (ìµœì´ˆ ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)")
OCR_READER = easyocr.Reader(['ko', 'en'], gpu=False, verbose=False)
log_print("EasyOCR ì´ˆê¸°í™” ì™„ë£Œ")

# MySQL DB ì„¤ì •
DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'localhost'),
    'user': os.getenv('DB_USER', 'root'),
    'password': os.getenv('DB_PASSWORD', ''),
    'database': os.getenv('DB_NAME', ''),
    'port': int(os.getenv('DB_PORT', 3306)),
    'charset': os.getenv('DB_CHARSET', 'utf8mb4'),
    'autocommit': os.getenv('DB_AUTOCOMMIT', 'False') == 'True',
    'use_pure': os.getenv('DB_USE_PURE', 'True') == 'True',
}

# =========================
# ìœ í‹¸ë¦¬í‹°
# =========================

def log(msg: str) -> None:
    print(f"[{datetime.now():%Y-%m-%d %H:%M:%S}] {msg}")


def get_headers() -> dict:
    return {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9",
        "Referer": BASE_URL,
    }


def clean_content(text: str) -> str:
    """ë‚´ìš© ì •ë¦¬: ê³¼ë„í•œ ì¤„ë°”ê¿ˆ ì œê±° ë° ë¬¸ë‹¨ ì •ë¦¬"""
    if not text:
        return ""

    # ì—°ì†ëœ ê³µë°±/íƒ­ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ
    text = re.sub(r'[ \t]+', ' ', text)

    # ì¤„ë°”ê¿ˆ ì •ë¦¬
    text = re.sub(r'([^\n])\n([^\n])', r'\1 \2', text)
    text = re.sub(r'\n{2,}', '\n\n', text)
    text = text.strip()

    return text


def classify_program_categories(title: str, content: str) -> List[str]:
    """í”„ë¡œê·¸ë¨ ì œëª©ê³¼ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜ (ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)"""
    text = (title + " " + content).lower()
    categories = []

    # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ íŒ¨í„´
    patterns = {
        "ë¹„êµê³¼": r"ë¹„êµê³¼",
        "ê³µëª¨ì „": r"ê³µëª¨ì „|ì½˜í…ŒìŠ¤íŠ¸|contest",
        "ë©˜í† ë§": r"ë©˜í† ë§|ë©˜í† |ë©˜í‹°",
        "ë´‰ì‚¬": r"ë´‰ì‚¬|ìì›ë´‰ì‚¬|volunteer",
        "ì·¨ì—…": r"ì·¨ì—…|ì±„ìš©|ë©´ì ‘|ì»¤ë¦¬ì–´|ì¸í„´|job|career|employment|ì…ì‚¬",
        "íƒë°©": r"íƒë°©|ê²¬í•™|íˆ¬ì–´|ë‹µì‚¬|field.?trip",
        "íŠ¹ê°•": r"íŠ¹ê°•|ê°•ì—°|ì„¸ë¯¸ë‚˜|ì›Œí¬ìƒµ|seminar|workshop",
    }

    # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë§¤ì¹­ í™•ì¸ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)
    for category, pattern in patterns.items():
        if re.search(pattern, text):
            categories.append(category)

    log(f"    âœ… ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜: {', '.join(categories) if categories else '(ì—†ìŒ)'}")
    return categories


def clean_and_extract_with_llm(title: str, raw_content: str) -> dict:
    """
    LLMì„ ì‚¬ìš©í•´ì„œ ê³µì§€ì‚¬í•­ ë‚´ìš©ì„ ì •ë¦¬í•˜ê³  í”„ë¡œê·¸ë¨ ì •ë³´ ì¶”ì¶œ

    Returns:
        {
            'cleaned_content': str,    # ì •ë¦¬ëœ ë³¸ë¬¸ ë‚´ìš©
            'target_department': str,  # "ì œí•œì—†ìŒ" ë˜ëŠ” "ì»´í“¨í„°ê³¼í•™ë¶€, ì „ìê³µí•™ê³¼"
            'target_grade': str,       # "ì œí•œì—†ìŒ" ë˜ëŠ” "1í•™ë…„, 2í•™ë…„"
            'application_start': str,  # "2025-11-01" ë˜ëŠ” None
            'application_end': str,    # "2025-11-30" ë˜ëŠ” None
            'operation_start': str,    # "2025-12-01" ë˜ëŠ” None
            'operation_end': str,      # "2025-12-15" ë˜ëŠ” None
            'capacity': int,           # 30 ë˜ëŠ” None
            'location': str,           # "ëŒ€ê°•ë‹¹" ë˜ëŠ” None
            'selection_method': str    # "ì„ ì°©ìˆœ" ë˜ëŠ” None
        }
    """
    if not OPENAI_CLIENT:
        log(f"    âš ï¸ OpenAI API ë¯¸ì„¤ì • - ê¸°ë³¸ê°’ ì‚¬ìš©")
        return {
            'cleaned_content': raw_content,
            'target_department': 'ì œí•œì—†ìŒ',
            'target_grade': 'ì œí•œì—†ìŒ',
            'application_start': None,
            'application_end': None,
            'operation_start': None,
            'operation_end': None,
            'capacity': None,
            'location': None,
            'selection_method': None
        }

    try:
        log(f"    LLMìœ¼ë¡œ ë‚´ìš© ì •ë¦¬ ë° ì •ë³´ ì¶”ì¶œ ì¤‘...")

        prompt = f"""ë‹¤ìŒì€ ëŒ€í•™êµ ê³µì§€ì‚¬í•­ì…ë‹ˆë‹¤. ë³¸ë¬¸ê³¼ OCRë¡œ ì¶”ì¶œëœ ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ê°€ ì„ì—¬ìˆì–´ ë§¥ë½ì´ ëŠê²¨ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì œëª©: {title}

ì›ë³¸ ë‚´ìš©:
{raw_content[:3000]}  # ì²˜ìŒ 3000ìë§Œ

---
**ì‘ì—… 1: ë‚´ìš© ì •ë¦¬**
- ë³¸ë¬¸ê³¼ ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ë¥¼ í†µí•©í•˜ì—¬ ë§¥ë½ìˆëŠ” í•˜ë‚˜ì˜ ê¸€ë¡œ ì¬êµ¬ì„±
- ì¤‘ë³µ ì •ë³´ëŠ” ì œê±°í•˜ë˜, ì¤‘ìš”í•œ ì •ë³´ëŠ” ëª¨ë‘ ìœ ì§€
- ì˜¤íƒ€ë‚˜ ë„ì–´ì“°ê¸° ì˜¤ë¥˜ ìˆ˜ì •
- ë¬¸ë‹¨ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì •ë¦¬ (í”„ë¡œê·¸ë¨ ê°œìš” â†’ ì¼ì • â†’ ëŒ€ìƒ â†’ ì‹ ì²­ë°©ë²• ìˆœì„œ)

**ì‘ì—… 2: ì •ë³´ ì¶”ì¶œ** (ì—†ìœ¼ë©´ null ë°˜í™˜):
1. target_department: ëŒ€ìƒ í•™ê³¼ (ì˜ˆ: "ì»´í“¨í„°ê³¼í•™ë¶€, ì „ìê³µí•™ê³¼" ë˜ëŠ” "ì œí•œì—†ìŒ")
2. target_grade: ëŒ€ìƒ í•™ë…„ (ì˜ˆ: "1í•™ë…„, 2í•™ë…„" ë˜ëŠ” "ì œí•œì—†ìŒ")
3. application_start: ì‹ ì²­ ì‹œì‘ì¼ (YYYY-MM-DD í˜•ì‹)
4. application_end: ì‹ ì²­ ë§ˆê°ì¼ (YYYY-MM-DD í˜•ì‹)
5. operation_start: ìš´ì˜/í–‰ì‚¬ ì‹œì‘ì¼ (YYYY-MM-DD í˜•ì‹)
6. operation_end: ìš´ì˜/í–‰ì‚¬ ì¢…ë£Œì¼ (YYYY-MM-DD í˜•ì‹)
7. capacity: ëª¨ì§‘ ì¸ì› (ìˆ«ìë§Œ, ì˜ˆ: 30)
8. location: ì¥ì†Œ (ì˜ˆ: "ëŒ€ê°•ë‹¹", "ì˜¨ë¼ì¸")
9. selection_method: ì„ ë°œ ë°©ì‹ (ì˜ˆ: "ì„ ì°©ìˆœ", "ì‹¬ì‚¬", "ì¶”ì²¨")

**ì¤‘ìš”:**
- ëŒ€ìƒì´ ëª…ì‹œë˜ì§€ ì•Šìœ¼ë©´ "ì œí•œì—†ìŒ"
- ë‚ ì§œëŠ” ë°˜ë“œì‹œ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜
- "ì‹ ì²­ ê¸°ê°„: 2025ë…„ 6ì›” 19ì¼ ~ 7ì›” 1ì¼" â†’ application_start: "2025-06-19", application_end: "2025-07-01"
- "ì ‘ìˆ˜ ê¸°ê°„: 2025.3.10(ì›”) - 3.20(ëª©)" â†’ application_start: "2025-03-10", application_end: "2025-03-20"
- ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ì´ ëª¨ë‘ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ë‘˜ ë‹¤ ì¶”ì¶œ
- JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€

JSON í˜•ì‹:
{{
    "cleaned_content": "ì •ë¦¬ëœ ë³¸ë¬¸ ë‚´ìš© (ëª¨ë“  ì¤‘ìš” ì •ë³´ í¬í•¨)",
    "target_department": "ì œí•œì—†ìŒ",
    "target_grade": "ì œí•œì—†ìŒ",
    "application_start": "2025-11-01",
    "application_end": "2025-11-30",
    "operation_start": null,
    "operation_end": null,
    "capacity": 30,
    "location": "ëŒ€ê°•ë‹¹",
    "selection_method": "ì„ ì°©ìˆœ"
}}
"""

        response = OPENAI_CLIENT.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ëŒ€í•™êµ ê³µì§€ì‚¬í•­ì„ ì •ë¦¬í•˜ê³  ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì›ë³¸ì˜ ì •ë³´ë¥¼ ìµœëŒ€í•œ ìœ ì§€í•˜ë©´ì„œ ë§¥ë½ìˆê²Œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
            max_tokens=2000,
            response_format={"type": "json_object"}
        )

        result_text = response.choices[0].message.content.strip()
        result = json.loads(result_text)

        log(f"    âœ… LLM ì •ë¦¬ ë° ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")

        # None ê°’ì„ ëª…ì‹œì ìœ¼ë¡œ ì²˜ë¦¬
        return {
            'cleaned_content': result.get('cleaned_content') or raw_content,
            'target_department': result.get('target_department') or 'ì œí•œì—†ìŒ',
            'target_grade': result.get('target_grade') or 'ì œí•œì—†ìŒ',
            'application_start': result.get('application_start'),
            'application_end': result.get('application_end'),
            'operation_start': result.get('operation_start'),
            'operation_end': result.get('operation_end'),
            'capacity': result.get('capacity'),
            'location': result.get('location'),
            'selection_method': result.get('selection_method')
        }

    except Exception as e:
        log(f"    âŒ LLM ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {
            'cleaned_content': raw_content,
            'target_department': 'ì œí•œì—†ìŒ',
            'target_grade': 'ì œí•œì—†ìŒ',
            'application_start': None,
            'application_end': None,
            'operation_start': None,
            'operation_end': None,
            'capacity': None,
            'location': None,
            'selection_method': None
        }


def extract_text_from_image(image_url: str) -> Optional[str]:
    """ì´ë¯¸ì§€ URLì—ì„œ OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    import base64

    try:
        log(f"    ì´ë¯¸ì§€ OCR ì‹œì‘: {image_url[:80]}...")

        # base64 ë°ì´í„° URI ì²´í¬
        if image_url.startswith('data:image'):
            # data:image/png;base64,iVBORw0KG... í˜•ì‹ ì²˜ë¦¬
            try:
                header, encoded = image_url.split(',', 1)
                image_data = base64.b64decode(encoded)
                log(f"    base64 ë°ì´í„° URI ë””ì½”ë”© ì™„ë£Œ")
            except Exception as e:
                log(f"    base64 ë””ì½”ë”© ì‹¤íŒ¨: {e}")
                return None
        else:
            # ì¼ë°˜ URLì—ì„œ ë‹¤ìš´ë¡œë“œ
            # ì™¸ë¶€ ë„ë©”ì¸ì¸ ê²½ìš° Referer ì²˜ë¦¬
            from urllib.parse import urlparse

            parsed_url = urlparse(image_url)
            if parsed_url.netloc and 'uos.ac.kr' not in parsed_url.netloc:
                # ì™¸ë¶€ ë„ë©”ì¸ - Refererë¥¼ í•´ë‹¹ ë„ë©”ì¸ìœ¼ë¡œ ì„¤ì •
                referer = f"{parsed_url.scheme}://{parsed_url.netloc}/"
            else:
                # ë‚´ë¶€ ë„ë©”ì¸ - BASE_URL ì‚¬ìš©
                referer = BASE_URL

            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
                "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
                "Referer": referer,
                "Sec-Fetch-Dest": "image",
                "Sec-Fetch-Mode": "no-cors",
                "Sec-Fetch-Site": "cross-site"
            }

            response = requests.get(
                image_url,
                headers=headers,
                timeout=(CONNECT_TIMEOUT, READ_TIMEOUT)
            )

            if response.status_code != 200:
                log(f"    ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: HTTP {response.status_code}")
                return None

            image_data = response.content

        # PIL Imageë¡œ ë³€í™˜
        image = Image.open(BytesIO(image_data))

        # RGBë¡œ ë³€í™˜ (RGBA ë“±ì˜ ê²½ìš° ëŒ€ë¹„)
        if image.mode != 'RGB':
            image = image.convert('RGB')

        # numpy arrayë¡œ ë³€í™˜
        image_np = np.array(image)

        # OCR ì‹¤í–‰
        log(f"    OCR ì‹¤í–‰ ì¤‘...")
        results = OCR_READER.readtext(image_np)

        # ê²°ê³¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì‹ ë¢°ë„ 0.3 ì´ìƒë§Œ)
        extracted_lines = []
        for (bbox, text, confidence) in results:
            if confidence > 0.3:
                extracted_lines.append(text)

        extracted_text = '\n'.join(extracted_lines)

        log(f"    OCR ì™„ë£Œ: {len(extracted_lines)}ê°œ í…ìŠ¤íŠ¸ ë¼ì¸ ì¶”ì¶œ")
        return extracted_text.strip()

    except Exception as e:
        log(f"    OCR ì‹¤íŒ¨: {e}")
        return None


def parse_departments(dept_text: str) -> list:
    """í•™ê³¼ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ í•™ê³¼ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    if not dept_text:
        return ['ì œí•œì—†ìŒ']

    dept_only = re.split(r'í•™ë…„', dept_text)[0].strip()
    departments = re.split(r'[,/]', dept_only)

    result = []
    for dept in departments:
        dept = dept.strip()
        dept = re.sub(r'[:ï¼š\s]+$', '', dept)
        if dept and dept not in ['ì œí•œì—†ìŒ', '']:
            result.append(dept)

    return result if result else ['ì œí•œì—†ìŒ']


def parse_grades(grade_text: str) -> list:
    """í•™ë…„ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ í•™ë…„ ì½”ë“œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (0: ì œí•œì—†ìŒ, 1-5: í•™ë…„, 6: ì¡¸ì—…ìƒ, 7: ëŒ€í•™ì›ìƒ)"""
    if not grade_text:
        return [0]

    grades = []
    numeric_grades = re.findall(r'(\d+)í•™ë…„', grade_text)
    for g in numeric_grades:
        grade_num = int(g)
        if 1 <= grade_num <= 5:
            grades.append(grade_num)

    if re.search(r'ì¡¸ì—…ìƒ?', grade_text):
        grades.append(6)
    if re.search(r'ëŒ€í•™ì›ìƒ?', grade_text):
        grades.append(7)
    if re.search(r'ì œí•œ\s*ì—†ìŒ|ì „ì²´', grade_text) or not grades:
        return [0]

    return grades


def get_db_connection():
    """MySQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒì„±"""
    try:
        connection = mysql.connector.connect(**DB_CONFIG)
        if connection.is_connected():
            return connection
    except Error as e:
        log(f"MySQL ì—°ê²° ì‹¤íŒ¨: {e}")
        return None


def insert_program_to_db(data: dict) -> str:
    """
    í”„ë¡œê·¸ë¨ ë°ì´í„°ë¥¼ DBì— ì‚½ì… (ê¸°ì¡´ sp_create_program ì‚¬ìš©)
    Returns: 'success', 'duplicate', 'error'
    """
    connection = None
    cursor = None

    try:
        connection = get_db_connection()
        if not connection:
            return 'error'

        cursor = connection.cursor()

        # [ê°±ì‹  ëª¨ë“œ] ì¤‘ë³µ ì‹œ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ì¬ì‚½ì…
        link = data.get('link', '')
        if link:
            check_query = "SELECT id FROM program WHERE link = %s LIMIT 1"
            cursor.execute(check_query, (link,))
            existing = cursor.fetchone()

            if existing:
                existing_id = existing[0]
                log(f"ğŸ”„ ê¸°ì¡´ ë°ì´í„° ë°œê²¬ (ID: {existing_id}) - ì‚­ì œ í›„ ì¬ì‚½ì…")

                # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (program_categoryëŠ” ON DELETE CASCADEë¡œ ìë™ ì‚­ì œë¨)
                cursor.execute("DELETE FROM program WHERE id = %s", (existing_id,))
                connection.commit()
                log(f"  âœ… ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")

        # í•™ê³¼ ë° í•™ë…„ íŒŒì‹±
        departments = parse_departments(data.get('target_department', ''))
        grades = parse_grades(data.get('target_grade', ''))
        categories = data.get('categories', [])

        # ì¤‘ë³µ ì œê±°
        departments = list(dict.fromkeys(departments))
        grades = list(dict.fromkeys(grades))
        categories = list(dict.fromkeys(categories))

        # JSON ë°ì´í„° ìƒì„±
        program_data = {
            'title': data.get('title', ''),
            'link': data.get('link', ''),
            'content': data.get('content', ''),
            'categories': categories,
            'departments': departments,
            'grades': grades
        }

        # ë‚ ì§œ í•„ë“œê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if data.get('posted_date'):
            program_data['app_start_date'] = data.get('posted_date')

        # JSON ë¬¸ìì—´ë¡œ ë³€í™˜
        json_data = json.dumps(program_data, ensure_ascii=False)

        # Stored Procedure í˜¸ì¶œ (OUT íŒŒë¼ë¯¸í„°)
        args = [json_data, 0]
        result_args = cursor.callproc('sp_create_program', args)
        connection.commit()

        # OUT íŒŒë¼ë¯¸í„°ì—ì„œ program_id ê°€ì ¸ì˜¤ê¸°
        program_id = result_args[1]

        log(f"âœ… DB ì‚½ì… ì„±ê³µ: {data.get('title', '')[:40]}... (ID: {program_id})")
        return 'success'

    except Error as e:
        log(f"âŒ DB ì‚½ì… ì‹¤íŒ¨: {e}")
        if connection:
            connection.rollback()
        return 'error'

    finally:
        if cursor:
            cursor.close()
        if connection and connection.is_connected():
            connection.close()


# =========================
# ëª©ë¡ ìˆ˜ì§‘
# =========================

def collect_notices_by_search(search_keyword: str, category_name: str, limit: int = NOTICES_PER_CATEGORY) -> List[Dict]:
    """
    ê²€ìƒ‰ì–´ë¡œ ê³µì§€ì‚¬í•­ ëª©ë¡ ìˆ˜ì§‘

    Args:
        search_keyword: ê²€ìƒ‰ì–´ (ì˜ˆ: "ê³µëª¨ì „")
        category_name: ì¹´í…Œê³ ë¦¬ ì´ë¦„ (ì˜ˆ: "ê³µëª¨ì „")
        limit: ìˆ˜ì§‘í•  ìµœëŒ€ ê°œìˆ˜

    Returns:
        [{"title": "...", "seq": "123", "category": "ê³µëª¨ì „"}, ...]
    """
    notices = []
    page = 1

    log(f"[{category_name}] ê²€ìƒ‰ ì‹œì‘: '{search_keyword}'")

    while len(notices) < limit and page <= MAX_PAGES:
        try:
            log(f"  í˜ì´ì§€ {page} ìš”ì²­ ì¤‘...")

            # ê²€ìƒ‰ íŒŒë¼ë¯¸í„° (ì‹¤ì œ URLê³¼ ë™ì¼í•˜ê²Œ êµ¬ì„±)
            params = {
                "list_id": "FA1",
                "seq": "0",
                "sort": "",
                "pageIndex": str(page),
                "searchCnd": "1",  # 1=ì œëª© ê²€ìƒ‰
                "searchWrd": search_keyword,
                "cate_id": "",
                "viewAuth": "Y",
                "writeAuth": "Y",  # Yë¡œ ë³€ê²½
                "board_list_num": "10",
                "lpageCount": "12",
                "menuid": "2000005009002000000",
                "identified": "anonymous"  # ìµëª… ì ‘ê·¼ í•„ìˆ˜!
            }

            response = requests.get(
                LIST_URL,
                params=params,
                headers=get_headers(),
                timeout=(CONNECT_TIMEOUT, READ_TIMEOUT)
            )

            if response.status_code != 200:
                log(f"  HTTP {response.status_code} ì—ëŸ¬")
                break

            soup = BeautifulSoup(response.text, 'html.parser')

            # ê²Œì‹œë¬¼ ëª©ë¡ ì°¾ê¸°: div.ti > a
            items = soup.select('div.ti > a')

            if not items:
                log(f"  ë” ì´ìƒ ê²Œì‹œë¬¼ ì—†ìŒ")
                break

            page_count = 0
            for item in items:
                if len(notices) >= limit:
                    break

                title = item.get_text(strip=True)
                href = item.get('href', '')

                # javascript:fnView('3', '30005'); ì—ì„œ seq ì¶”ì¶œ
                seq = None
                if 'fnView' in href:
                    match = re.search(r"fnView\(['\"](\d+)['\"]\s*,\s*['\"](\d+)['\"]\)", href)
                    if match:
                        seq = match.group(2)

                if seq and title:
                    # ë§í¬ ìƒì„± (ìƒì„¸ í˜ì´ì§€ URL)
                    link = f"{VIEW_URL}?identified=anonymous&list_id=FA1&seq={seq}"

                    notices.append({
                        'title': title,
                        'seq': seq,
                        'category': category_name,
                        'link': link
                    })
                    page_count += 1

            log(f"  í˜ì´ì§€ {page}: {page_count}ê°œ ìˆ˜ì§‘ (ëˆ„ì : {len(notices)}/{limit})")

            if page_count == 0:
                break

            page += 1
            # ìš”ì²­ ê°„ê²© ëŠ˜ë¦¬ê¸° (ì°¨ë‹¨ ë°©ì§€)
            time.sleep(random.uniform(2, 4))

        except Exception as e:
            log(f"  í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {e}")
            log(f"  10ì´ˆ ëŒ€ê¸° í›„ ê³„ì†...")
            time.sleep(10)  # ì°¨ë‹¨ í•´ì œ ëŒ€ê¸°
            break  # ì´ ì¹´í…Œê³ ë¦¬ëŠ” ì—¬ê¸°ì„œ ì¤‘ë‹¨

    log(f"[{category_name}] ìµœì¢… ìˆ˜ì§‘: {len(notices)}ê°œ")
    return notices


# =========================
# ìƒì„¸ í˜ì´ì§€ íŒŒì‹±
# =========================

def crawl_notice_detail(seq: str, categories: List[str]) -> Optional[Dict]:
    """
    ê³µì§€ì‚¬í•­ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ (Playwright ì‚¬ìš© - JavaScript ë Œë”ë§)

    Args:
        seq: ê³µì§€ì‚¬í•­ ë²ˆí˜¸
        categories: ì´ ê³µì§€ê°€ ì†í•œ ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸

    Returns:
        ê³µì§€ì‚¬í•­ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
    """
    try:
        url = f"{VIEW_URL}?identified=anonymous&list_id=FA1&seq={seq}"

        log(f"  ìƒì„¸ í˜ì´ì§€ ìš”ì²­: seq={seq}")

        # Playwrightë¡œ JavaScript ë Œë”ë§ í›„ HTML ê°€ì ¸ì˜¤ê¸°
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=True,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-dev-shm-usage',
                    '--no-sandbox'
                ]
            )

            context = browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080},
                locale='ko-KR',
                timezone_id='Asia/Seoul'
            )

            page = context.new_page()

            try:
                # í˜ì´ì§€ ë¡œë“œ
                page.goto(url, wait_until="domcontentloaded", timeout=30000)

                # JavaScript ì‹¤í–‰ ëŒ€ê¸°
                page.wait_for_timeout(3000)

                # HTML ê°€ì ¸ì˜¤ê¸°
                html = page.content()

            except Exception as e:
                log(f"  âš ï¸ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
                browser.close()
                return None

            browser.close()

        soup = BeautifulSoup(html, 'html.parser')

        # ì œëª© ì¶”ì¶œ
        title = None
        title_el = soup.select_one('div.vw-tibx h4')
        if title_el:
            title = title_el.get_text(strip=True)

        if not title:
            log(f"  ì œëª© ì—†ìŒ - ê±´ë„ˆëœ€")
            return None

        # ì‘ì„±ì¼ ë° ì‘ì„±ë¶€ì„œ ì¶”ì¶œ
        posted_date = None
        department = None

        spans = soup.select('div.vw-tibx div.da span')
        if len(spans) >= 3:
            department = spans[1].get_text(strip=True)
            date_text = spans[2].get_text(strip=True)
            # "2025-11-12" í˜•ì‹ ì¶”ì¶œ
            match = re.search(r'(\d{4}-\d{2}-\d{2})', date_text)
            if match:
                posted_date = match.group(1)

        # ë³¸ë¬¸ ì¶”ì¶œ (ìˆ˜ì •: div.vw-con ì‚¬ìš©)
        content = ""
        content_el = soup.select_one('div.vw-con')

        # ë””ë²„ê¹…: ë³¸ë¬¸ ìš”ì†Œ í™•ì¸
        if not content_el:
            log(f"  âš ï¸ div.vw-conì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            # ëŒ€ì•ˆ ì…€ë ‰í„° ì‹œë„
            content_el = soup.select_one('div.view-bx')
            if content_el:
                log(f"  âœ“ div.view-bx ì‚¬ìš©")

        if content_el:
            content = content_el.get_text("\n", strip=True)
            content = clean_content(content)
            log(f"  ë³¸ë¬¸ ê¸¸ì´: {len(content)}ê¸€ì")
        else:
            log(f"  âš ï¸ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

        # ì´ë¯¸ì§€ URL ì¶”ì¶œ (ë³¸ë¬¸ ë‚´ ì´ë¯¸ì§€)
        image_urls = []
        if content_el:
            for img in content_el.find_all('img'):
                img_src = img.get('src', '')
                if not img_src:
                    continue

                # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if img_src.startswith('http'):
                    img_url = img_src
                elif img_src.startswith('/'):
                    img_url = BASE_URL + img_src
                else:
                    img_url = BASE_URL + '/' + img_src

                # ì¤‘ë³µ ì œê±°
                if img_url not in image_urls:
                    image_urls.append(img_url)

        # ì´ë¯¸ì§€ OCR ì‹¤í–‰
        if image_urls:
            log(f"  ë³¸ë¬¸ ë‚´ ì´ë¯¸ì§€ {len(image_urls)}ê°œ ë°œê²¬")
            ocr_texts = []
            for idx, img_url in enumerate(image_urls, 1):
                log(f"    [{idx}/{len(image_urls)}] ì´ë¯¸ì§€ OCR ì²˜ë¦¬ ì¤‘...")
                ocr_text = extract_text_from_image(img_url)
                if ocr_text:
                    ocr_texts.append(f"[ì´ë¯¸ì§€ {idx} ì •ë³´]\n{ocr_text}")

            # OCR ê²°ê³¼ë¥¼ ë³¸ë¬¸ì— ì¶”ê°€
            if ocr_texts:
                log(f"  {len(ocr_texts)}ê°œ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ")
                content = content + "\n\n" + "\n\n".join(ocr_texts)
                content = clean_content(content)
            else:
                log(f"  ì´ë¯¸ì§€ëŠ” ìˆì§€ë§Œ OCRë¡œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")

        # ì²¨ë¶€íŒŒì¼ ì •ë³´ (ìˆìœ¼ë©´ ë³¸ë¬¸ì— ì¶”ê°€)
        attachments = []
        for file_link in soup.select('a[href*="board-download.do"]'):
            file_name = file_link.get_text(strip=True)
            if file_name and file_name not in attachments:
                attachments.append(file_name)

        if attachments:
            content += "\n\n[ì²¨ë¶€íŒŒì¼]\n" + "\n".join(f"- {f}" for f in attachments)

        # LLMìœ¼ë¡œ ë‚´ìš© ì •ë¦¬ ë° ì •ë³´ ì¶”ì¶œ (UOStory í˜•ì‹ì— ë§ì¶”ê¸°)
        llm_result = clean_and_extract_with_llm(title, content)

        # ì •ë¦¬ëœ ë‚´ìš© ì‚¬ìš©
        cleaned_content = llm_result['cleaned_content']

        # ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜ (ì œëª© + ì •ë¦¬ëœ ë³¸ë¬¸ ê¸°ë°˜, ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)
        auto_categories = classify_program_categories(title, cleaned_content)

        # ê²€ìƒ‰ ì¹´í…Œê³ ë¦¬ì™€ ìë™ ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ ë³‘í•© (ì¤‘ë³µ ì œê±°)
        combined_categories = list(dict.fromkeys(categories + auto_categories))

        return {
            'title': title,
            'link': url,
            'content': cleaned_content,  # LLMì´ ì •ë¦¬í•œ ë‚´ìš© ì‚¬ìš©
            'categories': combined_categories,  # ë³‘í•©ëœ ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸
            'posted_date': posted_date,
            'department': department,
            'seq': seq,
            # LLMìœ¼ë¡œ ì¶”ì¶œí•œ ì •ë³´ ì¶”ê°€
            'target_department': llm_result['target_department'],
            'target_grade': llm_result['target_grade'],
            'application_start': llm_result['application_start'],
            'application_end': llm_result['application_end'],
            'operation_start': llm_result['operation_start'],
            'operation_end': llm_result['operation_end'],
            'capacity': llm_result['capacity'],
            'location': llm_result['location'],
            'selection_method': llm_result['selection_method']
        }

    except Exception as e:
        log(f"  ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None


# =========================
# ë©”ì¸
# =========================

def print_program_info(data: dict, idx: int) -> None:
    """uostory_crawlerì™€ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ í”„ë¡œê·¸ë¨ ì •ë³´ ì¶œë ¥ + í”„ë¡œì‹œì € í˜¸ì¶œ í˜•ì‹"""
    print("\n" + "="*80)
    print(f"[{idx}] í”„ë¡œê·¸ë¨ ì •ë³´")
    print(f"ì œëª©: {data['title']}")
    print(f"ë§í¬: {data.get('link', '')}")
    print("-"*80)

    # ì¹´í…Œê³ ë¦¬ (ë¦¬ìŠ¤íŠ¸ í˜•ì‹)
    if data.get('categories'):
        print(f"ì¹´í…Œê³ ë¦¬: {', '.join(data['categories'])}")

    # ëŒ€ìƒ í•™ê³¼ ë° í•™ë…„
    if data.get('target_department'):
        print(f"ëŒ€ìƒí•™ê³¼: {data['target_department']}")
    if data.get('target_grade'):
        print(f"ëŒ€ìƒí•™ë…„: {data['target_grade']}")

    # ì„ ë°œë°©ì‹, ëª¨ì§‘ì¸ì›, ì¥ì†Œ
    if data.get('selection_method'):
        print(f"ì„ ë°œë°©ì‹: {data['selection_method']}")
    if data.get('capacity'):
        print(f"ëª¨ì§‘ì¸ì›: {data['capacity']}ëª…")
    if data.get('location'):
        print(f"ì¥ì†Œ: {data['location']}")

    print("-"*80)

    # ë‚ ì§œ ì •ë³´
    if data.get('application_start'):
        print(f"ì‹ ì²­ ì‹œì‘: {data['application_start']}")
    if data.get('application_end'):
        print(f"ì‹ ì²­ ë§ˆê°: {data['application_end']}")
    if data.get('operation_start'):
        print(f"ìš´ì˜ ì‹œì‘: {data['operation_start']}")
    if data.get('operation_end'):
        print(f"ìš´ì˜ ë§ˆê°: {data['operation_end']}")
    if data.get('posted_date'):
        print(f"ì‘ì„±ì¼: {data['posted_date']}")

    print("-"*80)

    # ë³¸ë¬¸ ë‚´ìš©
    if data.get('content'):
        cleaned = clean_content(data['content'])
        print(f"ë‚´ìš©:\n{cleaned}")

    print("-"*80)

    # í”„ë¡œì‹œì € í˜¸ì¶œ í˜•ì‹ ì¶œë ¥
    departments = parse_departments(data.get('target_department', ''))
    grades = parse_grades(data.get('target_grade', ''))

    program_data = {
        'title': data.get('title', ''),
        'link': data.get('link', ''),
        'content': data.get('content', ''),
        'categories': data.get('categories', []),
        'departments': departments,
        'grades': grades
    }

    # ë‚ ì§œ í•„ë“œ ì¶”ê°€
    if data.get('application_start'):
        program_data['app_start_date'] = data.get('application_start')
    if data.get('application_end'):
        program_data['app_end_date'] = data.get('application_end')

    # JSON ì¶œë ¥
    json_str = json.dumps(program_data, ensure_ascii=False, indent=2)
    print(f"\ní”„ë¡œì‹œì € í˜¸ì¶œ í˜•ì‹:")
    print(f"SET @p = '{json_str}';")
    

    print("="*80 + "\n")


def main():
    log("="*60)
    log("ì„œìš¸ì‹œë¦½ëŒ€ í¬í„¸ ê³µì§€ì‚¬í•­ ê²€ìƒ‰ ê¸°ë°˜ í¬ë¡¤ëŸ¬")
    log("="*60)
    print()

    all_notices = []
    inserted_count = 0
    duplicate_count = 0
    merged_count = 0
    error_count = 0

    # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê²€ìƒ‰ ë° í¬ë¡¤ë§
    for category in SEARCH_CATEGORIES:
        log(f"\n{'='*60}")
        log(f"ì¹´í…Œê³ ë¦¬: [{category['name']}]")
        log(f"{'='*60}")

        # 1. ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì—ì„œ ëª©ë¡ ìˆ˜ì§‘
        notices = collect_notices_by_search(
            search_keyword=category['keyword'],
            category_name=category['name'],
            limit=NOTICES_PER_CATEGORY
        )

        if not notices:
            log(f"[{category['name']}] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            continue

        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ëœë¤ ìƒ˜í”Œë§
        if TEST_RANDOM_SAMPLE is not None and len(notices) > TEST_RANDOM_SAMPLE:
            log(f"[í…ŒìŠ¤íŠ¸ ëª¨ë“œ] {len(notices)}ê°œ ì¤‘ ëœë¤ {TEST_RANDOM_SAMPLE}ê°œ ìƒ˜í”Œë§")
            notices = random.sample(notices, TEST_RANDOM_SAMPLE)
        # ì‹¤ì œ ìš´ì˜ ëª¨ë“œ: ì „ì²´ í¬ë¡¤ë§
        # notices = notices  # ê·¸ëŒ€ë¡œ ì‚¬ìš©

        # 2. ê° ê³µì§€ì‚¬í•­ ìƒì„¸ í¬ë¡¤ë§ ë° ì¶œë ¥
        for idx, notice in enumerate(notices, 1):
            log(f"\n[{category['name']} {idx}/{len(notices)}] ì²˜ë¦¬ ì¤‘...")

            # [ê°±ì‹  ëª¨ë“œ] í¬ë¡¤ë§ ì „ ì¤‘ë³µ ì²´í¬ ë¹„í™œì„±í™” - ëª¨ë“  ê³µì§€ í¬ë¡¤ë§
            # link = notice['link']
            #
            # # DB ì—°ê²°í•´ì„œ ì²´í¬
            # connection = get_db_connection()
            # if connection:
            #     try:
            #         cursor = connection.cursor()
            #         check_query = "SELECT id FROM program WHERE link = %s LIMIT 1"
            #         cursor.execute(check_query, (link,))
            #         existing = cursor.fetchone()
            #
            #         if existing:
            #             log(f"  â­ DBì— ì´ë¯¸ ì¡´ì¬ (ID: {existing[0]}) - í¬ë¡¤ë§ ê±´ë„ˆë›°ê¸°")
            #             duplicate_count += 1
            #             cursor.close()
            #             connection.close()
            #             continue  # ë‹¤ìŒ ê³µì§€ë¡œ
            #
            #         cursor.close()
            #         connection.close()
            #
            #     except Error as e:
            #         log(f"  âš ï¸ DB ì²´í¬ ì‹¤íŒ¨: {e}")
            #         if connection:
            #             connection.close()

            # ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ (DBì— ì—†ëŠ” ê²ƒë§Œ)
            data = crawl_notice_detail(
                seq=notice['seq'],
                categories=[notice['category']]
            )

            if data:
                all_notices.append(data)

                # uostory_crawlerì™€ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ìƒì„¸ ì •ë³´ ì¶œë ¥
                print_program_info(data, len(all_notices))

                # DB ì‚½ì…
                result = insert_program_to_db(data)
                if result == 'success':
                    inserted_count += 1
                elif result == 'merged':
                    merged_count += 1
                elif result == 'duplicate':
                    duplicate_count += 1
                elif result == 'error':
                    error_count += 1
            else:
                error_count += 1

            # ë‹¤ìŒ ìš”ì²­ ì „ ëŒ€ê¸°
            if idx < len(notices):
                sleep_time = random.uniform(REQUEST_SLEEP_MIN, REQUEST_SLEEP_MAX)
                log(f"  {sleep_time:.1f}ì´ˆ ëŒ€ê¸°...")
                time.sleep(sleep_time)

    # ìµœì¢… í†µê³„ (uostory_crawlerì™€ ë™ì¼í•œ í˜•ì‹)
    log(f"\n{'='*60}")
    log(f"í¬ë¡¤ë§ ì™„ë£Œ í†µê³„:")
    log(f"  - ì´ ìˆ˜ì§‘: {len(all_notices)}ê°œ")
    log(f"  - DB ì‚½ì…: {inserted_count}ê°œ")
    log(f"  - ì¹´í…Œê³ ë¦¬ ë³‘í•©: {merged_count}ê°œ")
    log(f"  - ì¤‘ë³µ ê±´ë„ˆëœ€: {duplicate_count}ê°œ")
    if error_count > 0:
        log(f"  - ì²˜ë¦¬ ì‹¤íŒ¨: {error_count}ê°œ")
    log(f"{'='*60}")

    # JSON íŒŒì¼ë¡œë„ ì €ì¥
    output_file = "portal_notices.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(all_notices, f, ensure_ascii=False, indent=2)
    log(f"{output_file}ì— ì €ì¥ ì™„ë£Œ")

    log("\nâœ… ì™„ë£Œ!")
    return 0


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        log("\nì¤‘ë‹¨ë¨")
        exit(0)
    except Exception as e:
        log(f"ERROR: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
